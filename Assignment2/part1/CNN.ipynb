{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Digit Classification with CNN Variants Based on LeNet-5\n",
    "\n",
    "This notebook implements a digit classification task using the Reduced MNIST dataset. We explore various Convolutional Neural Network (CNN) architectures based on the LeNet-5 model, testing modifications such as different activation functions, layer configurations, pooling methods, regularization techniques, optimizers, and kernel sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-24 22:05:47.626267: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-03-24 22:05:47.630164: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-03-24 22:05:47.642787: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1742846747.663334    6974 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1742846747.671004    6974 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1742846747.688834    6974 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1742846747.688861    6974 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1742846747.688863    6974 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1742846747.688865    6974 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-03-24 22:05:47.694270: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import cv2\n",
    "import keras\n",
    "import time\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, Flatten, AveragePooling2D, MaxPooling2D, Dropout, BatchNormalization\n",
    "from keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam, SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Preprocessing\n",
    "\n",
    "In this section, we load the Reduced MNIST dataset from the specified training and testing directories. The images are grayscale (28x28 pixels), and the folder names (0-9) serve as labels. We shuffle the data to ensure randomness and reshape it for CNN input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training class directories: ['0', '7', '5', '2', '6', '8', '4', '9', '3', '1']\n",
      "Testing class directories: ['0', '7', '5', '2', '6', '8', '4', '9', '3', '1']\n",
      "Training images shape: (10000, 28, 28)\n",
      "Training labels shape: (10000,)\n",
      "Testing images shape: (2000, 28, 28)\n",
      "Testing labels shape: (2000,)\n"
     ]
    }
   ],
   "source": [
    "# Define paths to training and testing data directories\n",
    "train_data_dir = './Reduced MNIST Data/Reduced Training data'\n",
    "test_data_dir = './Reduced MNIST Data/Reduced Testing data'\n",
    "\n",
    "# Get list of subdirectories (each representing a digit class)\n",
    "train_class_dirs = os.listdir(train_data_dir)\n",
    "test_class_dirs = os.listdir(test_data_dir)\n",
    "\n",
    "print(\"Training class directories:\", train_class_dirs)\n",
    "print(\"Testing class directories:\", test_class_dirs)\n",
    "\n",
    "# Initialize lists to store images and labels\n",
    "train_images = []\n",
    "train_labels = []\n",
    "\n",
    "# Load training data\n",
    "for digit_class in train_class_dirs:\n",
    "    class_path = os.path.join(train_data_dir, digit_class)\n",
    "    for image_file in os.listdir(class_path):\n",
    "        image_path = os.path.join(class_path, image_file)\n",
    "        # Read image in grayscale (0-255 pixel values)\n",
    "        image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "        train_images.append(image)\n",
    "        train_labels.append(digit_class)  # Folder name is the label\n",
    "\n",
    "# Convert lists to NumPy arrays\n",
    "train_images = np.array(train_images)\n",
    "train_labels = np.array(train_labels)\n",
    "\n",
    "print(\"Training images shape:\", train_images.shape)\n",
    "print(\"Training labels shape:\", train_labels.shape)\n",
    "\n",
    "# Load testing data\n",
    "test_images = []\n",
    "test_labels = []\n",
    "\n",
    "for digit_class in test_class_dirs:\n",
    "    class_path = os.path.join(test_data_dir, digit_class)\n",
    "    for image_file in os.listdir(class_path):\n",
    "        image_path = os.path.join(class_path, image_file)\n",
    "        image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "        test_images.append(image)\n",
    "        test_labels.append(digit_class)\n",
    "\n",
    "# Convert lists to NumPy arrays\n",
    "test_images = np.array(test_images)\n",
    "test_labels = np.array(test_labels)\n",
    "\n",
    "print(\"Testing images shape:\", test_images.shape)\n",
    "print(\"Testing labels shape:\", test_labels.shape)\n",
    "\n",
    "# Shuffle training and testing data for randomness\n",
    "train_images, train_labels = shuffle(train_images, train_labels, random_state=4)\n",
    "test_images, test_labels = shuffle(test_images, test_labels, random_state=4)\n",
    "\n",
    "# Reshape data for CNN input (add channel dimension)\n",
    "def reshape_for_cnn(images_train, images_test):\n",
    "    \"\"\"Reshape image arrays to include a channel dimension for CNN input.\"\"\"\n",
    "    train_cnn_input = images_train.reshape(images_train.shape[0], 28, 28, 1)\n",
    "    test_cnn_input = images_test.reshape(images_test.shape[0], 28, 28, 1)\n",
    "    return train_cnn_input, test_cnn_input\n",
    "\n",
    "train_cnn_input, test_cnn_input = reshape_for_cnn(train_images, test_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model Training and Evaluation Functions\n",
    "\n",
    "We define helper functions to train and evaluate CNN models, measuring training time, testing time, and accuracy. These functions will be used across all model variants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_cnn(model, train_data, train_labels, test_data, test_labels, \n",
    "                          epochs=10, batch_size=64, use_early_stopping=False, verbose=0):\n",
    "    \"\"\"Train and evaluate a CNN model, returning performance metrics.\"\"\"\n",
    "    # Set up callbacks (e.g., early stopping)\n",
    "    callbacks = []\n",
    "    if use_early_stopping:\n",
    "        early_stop = EarlyStopping(monitor='accuracy', patience=3, restore_best_weights=True)\n",
    "        callbacks.append(early_stop)\n",
    "    \n",
    "    # Train the model and measure training time\n",
    "    start_time = time.time()\n",
    "    history = model.fit(\n",
    "        train_data, \n",
    "        to_categorical(train_labels), \n",
    "        epochs=epochs, \n",
    "        batch_size=batch_size,\n",
    "        shuffle=True, \n",
    "        verbose=verbose, \n",
    "        callbacks=callbacks\n",
    "    )\n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    # Evaluate the model and measure testing time\n",
    "    start_time = time.time()\n",
    "    test_loss, test_accuracy = model.evaluate(test_data, to_categorical(test_labels), verbose=0)\n",
    "    testing_time = (time.time() - start_time) * 1000  # Convert to milliseconds\n",
    "    \n",
    "    return training_time, testing_time, test_accuracy, history\n",
    "\n",
    "def print_performance_metrics(model_name, training_time, testing_time, test_accuracy):\n",
    "    \"\"\"Display performance metrics in a formatted manner.\"\"\"\n",
    "    print(f\"----- {model_name} -----\")\n",
    "    print(f\"Training Time: {training_time:.2f} seconds\")\n",
    "    print(f\"Testing Time: {testing_time:.2f} milliseconds\")\n",
    "    print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. CNN Model Variants\n",
    "\n",
    "We define multiple CNN architectures based on LeNet-5, each with a specific modification:\n",
    "- Base LeNet-5 model\n",
    "- Increased number of filters\n",
    "- Tanh activation function\n",
    "- ELU activation function\n",
    "- Fewer layers\n",
    "- Additional layer\n",
    "- MaxPooling instead of AveragePooling\n",
    "- Dropout regularization\n",
    "- Batch normalization\n",
    "- SGD optimizer with momentum\n",
    "- Smaller kernel size (3x3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base LeNet-5 Model\n",
    "def create_base_lenet5():\n",
    "    \"\"\"Create the original LeNet-5 architecture.\"\"\"\n",
    "    model = Sequential([\n",
    "        Conv2D(6, (5, 5), activation='relu', input_shape=(28, 28, 1), padding='valid'),\n",
    "        AveragePooling2D(pool_size=(2, 2), strides=(2, 2), padding='valid'),\n",
    "        Conv2D(16, (5, 5), activation='relu', padding='valid'),\n",
    "        AveragePooling2D(pool_size=(2, 2), strides=(2, 2), padding='valid'),\n",
    "        Flatten(),\n",
    "        Dense(120, activation='relu'),\n",
    "        Dense(84, activation='relu'),\n",
    "        Dense(10, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Variant 1: Increased Number of Filters\n",
    "def create_increased_filters():\n",
    "    \"\"\"Increase the number of filters in convolutional layers.\"\"\"\n",
    "    model = Sequential([\n",
    "        Conv2D(18, (5, 5), activation='relu', input_shape=(28, 28, 1)),\n",
    "        AveragePooling2D((2, 2)),\n",
    "        Conv2D(24, (5, 5), activation='relu'),\n",
    "        AveragePooling2D((2, 2)),\n",
    "        Flatten(),\n",
    "        Dense(120, activation='relu'),\n",
    "        Dense(84, activation='relu'),\n",
    "        Dense(10, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Variant 2: Tanh Activation Function\n",
    "def create_tanh_activation():\n",
    "    \"\"\"Use tanh activation instead of ReLU.\"\"\"\n",
    "    model = Sequential([\n",
    "        Conv2D(6, (5, 5), activation='tanh', input_shape=(28, 28, 1)),\n",
    "        AveragePooling2D((2, 2)),\n",
    "        Conv2D(16, (5, 5), activation='tanh'),\n",
    "        AveragePooling2D((2, 2)),\n",
    "        Flatten(),\n",
    "        Dense(120, activation='tanh'),\n",
    "        Dense(84, activation='tanh'),\n",
    "        Dense(10, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Variant 3: ELU Activation Function\n",
    "def create_elu_activation():\n",
    "    \"\"\"Use ELU activation instead of ReLU.\"\"\"\n",
    "    model = Sequential([\n",
    "        Conv2D(6, (5, 5), activation='elu', input_shape=(28, 28, 1)),\n",
    "        AveragePooling2D((2, 2)),\n",
    "        Conv2D(16, (5, 5), activation='elu'),\n",
    "        AveragePooling2D((2, 2)),\n",
    "        Flatten(),\n",
    "        Dense(120, activation='elu'),\n",
    "        Dense(84, activation='elu'),\n",
    "        Dense(10, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Variant 4: Fewer Layers\n",
    "def create_fewer_layers():\n",
    "    \"\"\"Remove one dense layer from the base model.\"\"\"\n",
    "    model = Sequential([\n",
    "        Conv2D(6, (5, 5), activation='relu', input_shape=(28, 28, 1)),\n",
    "        AveragePooling2D((2, 2)),\n",
    "        Conv2D(16, (5, 5), activation='relu'),\n",
    "        AveragePooling2D((2, 2)),\n",
    "        Flatten(),\n",
    "        Dense(84, activation='relu'),  # Removed 120-unit layer\n",
    "        Dense(10, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Variant 5: Additional Layer\n",
    "def create_additional_layer():\n",
    "    \"\"\"Add an extra dense layer to the base model.\"\"\"\n",
    "    model = Sequential([\n",
    "        Conv2D(6, (5, 5), activation='relu', input_shape=(28, 28, 1)),\n",
    "        AveragePooling2D((2, 2)),\n",
    "        Conv2D(16, (5, 5), activation='relu'),\n",
    "        AveragePooling2D((2, 2)),\n",
    "        Flatten(),\n",
    "        Dense(120, activation='relu'),\n",
    "        Dense(84, activation='relu'),\n",
    "        Dense(42, activation='relu'),  # Additional layer\n",
    "        Dense(10, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Variant 6: MaxPooling Instead of AveragePooling\n",
    "def create_max_pooling():\n",
    "    \"\"\"Replace AveragePooling with MaxPooling.\"\"\"\n",
    "    model = Sequential([\n",
    "        Conv2D(6, (5, 5), activation='relu', input_shape=(28, 28, 1)),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Conv2D(16, (5, 5), activation='relu'),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Flatten(),\n",
    "        Dense(120, activation='relu'),\n",
    "        Dense(84, activation='relu'),\n",
    "        Dense(10, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Variant 7: Dropout Regularization\n",
    "def create_dropout_regularization():\n",
    "    \"\"\"Add dropout layers for regularization.\"\"\"\n",
    "    model = Sequential([\n",
    "        Conv2D(6, (5, 5), activation='relu', input_shape=(28, 28, 1)),\n",
    "        AveragePooling2D((2, 2)),\n",
    "        Dropout(0.25),\n",
    "        Conv2D(16, (5, 5), activation='relu'),\n",
    "        AveragePooling2D((2, 2)),\n",
    "        Dropout(0.25),\n",
    "        Flatten(),\n",
    "        Dense(120, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(84, activation='relu'),\n",
    "        Dense(10, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Variant 8: Batch Normalization\n",
    "def create_batch_normalization():\n",
    "    \"\"\"Add batch normalization layers.\"\"\"\n",
    "    model = Sequential([\n",
    "        Conv2D(6, (5, 5), activation='relu', input_shape=(28, 28, 1)),\n",
    "        BatchNormalization(),\n",
    "        AveragePooling2D((2, 2)),\n",
    "        Conv2D(16, (5, 5), activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        AveragePooling2D((2, 2)),\n",
    "        Flatten(),\n",
    "        Dense(120, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dense(84, activation='relu'),\n",
    "        Dense(10, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Variant 9: SGD Optimizer with Momentum\n",
    "def create_sgd_optimizer():\n",
    "    \"\"\"Use SGD optimizer with momentum instead of Adam.\"\"\"\n",
    "    model = Sequential([\n",
    "        Conv2D(6, (5, 5), activation='relu', input_shape=(28, 28, 1)),\n",
    "        AveragePooling2D((2, 2)),\n",
    "        Conv2D(16, (5, 5), activation='relu'),\n",
    "        AveragePooling2D((2, 2)),\n",
    "        Flatten(),\n",
    "        Dense(120, activation='relu'),\n",
    "        Dense(84, activation='relu'),\n",
    "        Dense(10, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer=SGD(learning_rate=0.01, momentum=0.9), \n",
    "                 loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Variant 10: Smaller Kernel Size (3x3)\n",
    "def create_smaller_kernel():\n",
    "    \"\"\"Use 3x3 kernels instead of 5x5.\"\"\"\n",
    "    model = Sequential([\n",
    "        Conv2D(6, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
    "        AveragePooling2D((2, 2)),\n",
    "        Conv2D(16, (3, 3), activation='relu'),\n",
    "        AveragePooling2D((2, 2)),\n",
    "        Flatten(),\n",
    "        Dense(120, activation='relu'),\n",
    "        Dense(84, activation='relu'),\n",
    "        Dense(10, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Experiment Runner and Results\n",
    "\n",
    "We run experiments for each CNN variant, training them on the dataset and evaluating their performance based on training time, testing time, and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rero/.local/lib/python3.10/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "E0000 00:00:1742846865.086843    6974 cuda_executor.cc:1228] INTERNAL: CUDA Runtime error: Failed call to cudaGetRuntimeVersion: Error loading CUDA libraries. GPU will not be used.: Error loading CUDA libraries. GPU will not be used.\n",
      "W0000 00:00:1742846865.088040    6974 gpu_device.cc:2341] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2025-03-24 22:07:46.298413: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 25171200 exceeds 10% of free system memory.\n",
      "2025-03-24 22:07:46.308774: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 25171200 exceeds 10% of free system memory.\n",
      "2025-03-24 22:07:46.316571: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 25171200 exceeds 10% of free system memory.\n",
      "2025-03-24 22:07:46.323499: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 25171200 exceeds 10% of free system memory.\n",
      "2025-03-24 22:07:46.329902: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 25171200 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Base LeNet-5 Model -----\n",
      "Training Time: 12.45 seconds\n",
      "Testing Time: 339.65 milliseconds\n",
      "Test Accuracy: 96.20%\n",
      "\n",
      "\n",
      "----- Increased Number of Filters -----\n",
      "Training Time: 20.08 seconds\n",
      "Testing Time: 413.23 milliseconds\n",
      "Test Accuracy: 97.35%\n",
      "\n",
      "\n",
      "----- Tanh Activation Function -----\n",
      "Training Time: 12.35 seconds\n",
      "Testing Time: 417.19 milliseconds\n",
      "Test Accuracy: 96.90%\n",
      "\n",
      "\n",
      "----- ELU Activation Function -----\n",
      "Training Time: 12.88 seconds\n",
      "Testing Time: 312.66 milliseconds\n",
      "Test Accuracy: 97.75%\n",
      "\n",
      "\n",
      "----- Fewer Layers -----\n",
      "Training Time: 10.32 seconds\n",
      "Testing Time: 288.19 milliseconds\n",
      "Test Accuracy: 97.05%\n",
      "\n",
      "\n",
      "----- Additional Layer -----\n",
      "Training Time: 10.73 seconds\n",
      "Testing Time: 301.06 milliseconds\n",
      "Test Accuracy: 97.15%\n",
      "\n",
      "\n",
      "----- MaxPooling Instead of AveragePooling -----\n",
      "Training Time: 10.85 seconds\n",
      "Testing Time: 306.11 milliseconds\n",
      "Test Accuracy: 96.25%\n",
      "\n",
      "\n",
      "----- Dropout Regularization -----\n",
      "Training Time: 11.61 seconds\n",
      "Testing Time: 296.35 milliseconds\n",
      "Test Accuracy: 98.05%\n",
      "\n",
      "\n",
      "----- Batch Normalization -----\n",
      "Training Time: 16.78 seconds\n",
      "Testing Time: 392.57 milliseconds\n",
      "Test Accuracy: 98.25%\n",
      "\n",
      "\n",
      "----- SGD Optimizer with Momentum -----\n",
      "Training Time: 9.78 seconds\n",
      "Testing Time: 298.33 milliseconds\n",
      "Test Accuracy: 93.50%\n",
      "\n",
      "\n",
      "----- Smaller Kernel Size (3x3) -----\n",
      "Training Time: 9.60 seconds\n",
      "Testing Time: 321.04 milliseconds\n",
      "Test Accuracy: 97.60%\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def run_cnn_experiments(train_images, train_labels, test_images, test_labels):\n",
    "    \"\"\"Run experiments for all CNN variants and collect results.\"\"\"\n",
    "    # Reshape data for CNN input\n",
    "    train_cnn_data, test_cnn_data = reshape_for_cnn(train_images, test_images)\n",
    "    \n",
    "    # Define all model variants\n",
    "    model_variants = {\n",
    "        \"Base LeNet-5 Model\": create_base_lenet5,\n",
    "        \"Increased Number of Filters\": create_increased_filters,\n",
    "        \"Tanh Activation Function\": create_tanh_activation,\n",
    "        \"ELU Activation Function\": create_elu_activation,\n",
    "        \"Fewer Layers\": create_fewer_layers,\n",
    "        \"Additional Layer\": create_additional_layer,\n",
    "        \"MaxPooling Instead of AveragePooling\": create_max_pooling,\n",
    "        \"Dropout Regularization\": create_dropout_regularization,\n",
    "        \"Batch Normalization\": create_batch_normalization,\n",
    "        \"SGD Optimizer with Momentum\": create_sgd_optimizer,\n",
    "        \"Smaller Kernel Size (3x3)\": create_smaller_kernel\n",
    "    }\n",
    "    \n",
    "    # Store results\n",
    "    experiment_results = []\n",
    "    \n",
    "    # Run each experiment\n",
    "    for variant_name, create_model_fn in model_variants.items():\n",
    "        model = create_model_fn()\n",
    "        training_time, testing_time, test_accuracy, history = train_and_evaluate_cnn(\n",
    "            model, train_cnn_data, train_labels, test_cnn_data, test_labels\n",
    "        )\n",
    "        print_performance_metrics(variant_name, training_time, testing_time, test_accuracy)\n",
    "        print(\"\\n\")\n",
    "        experiment_results.append({\n",
    "            \"name\": variant_name,\n",
    "            \"training_time\": training_time,\n",
    "            \"testing_time\": testing_time,\n",
    "            \"test_accuracy\": test_accuracy\n",
    "        })\n",
    "    \n",
    "    return experiment_results\n",
    "\n",
    "# Execute experiments\n",
    "results = run_cnn_experiments(train_images, train_labels, test_images, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Conclusion\n",
    "\n",
    "This notebook evaluates multiple CNN architectures derived from LeNet-5 on the Reduced MNIST dataset. By comparing training time, testing time, and accuracy across variants, we can assess the impact of architectural changes such as activation functions, layer counts, pooling methods, regularization, optimizers, and kernel sizes on model performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
