# -*- coding: utf-8 -*-
"""problem2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12fDJjpwkaryUW1BO4FqOkwt0Vsbt7zdR
"""

!pip install datasets

!pip install evaluate

"""
Enhanced Buckwalter Transliteration for Arabic
- Includes standard Buckwalter mapping
- Adds extended Buckwalter characters
- Includes XML-safe variant mappings
- Handles special cases and diacritics properly
"""

# Standard Buckwalter → Arabic mapping
BUCKWALTER_TO_ARABIC = {
    "'": "\u0621",  # hamza-on-the-line
    "|": "\u0622",  # madda-on-alif
    ">": "\u0623",  # hamza-on-alif
    "&": "\u0624",  # hamza-on-waw
    "<": "\u0625",  # hamza-under-alif
    "}": "\u0626",  # hamza-on-ya
    "A": "\u0627",  # alif
    "b": "\u0628",  # ba
    "p": "\u0629",  # ta marbuta
    "t": "\u062A",  # ta
    "v": "\u062B",  # tha
    "j": "\u062C",  # jim
    "H": "\u062D",  # ha (voiceless pharyngeal fricative)
    "x": "\u062E",  # kha
    "d": "\u062F",  # dal
    "*": "\u0630",  # dhal
    "r": "\u0631",  # ra
    "z": "\u0632",  # zay
    "s": "\u0633",  # sin
    "$": "\u0634",  # shin
    "S": "\u0635",  # sad
    "D": "\u0636",  # dad
    "T": "\u0637",  # ta (emphatic)
    "Z": "\u0638",  # za (emphatic)
    "E": "\u0639",  # ayn
    "g": "\u063A",  # ghayn
    "_": "\u0640",  # tatweel (kashida)
    "f": "\u0641",  # fa
    "q": "\u0642",  # qaf
    "k": "\u0643",  # kaf
    "l": "\u0644",  # lam
    "m": "\u0645",  # mim
    "n": "\u0646",  # nun
    "h": "\u0647",  # ha
    "w": "\u0648",  # waw
    "Y": "\u0649",  # alif maqṣūra
    "y": "\u064A",  # ya
    "F": "\u064B",  # fathatān
    "N": "\u064C",  # ḍammatān
    "K": "\u064D",  # kasratān
    "a": "\u064E",  # fatha
    "u": "\u064F",  # ḍamma
    "i": "\u0650",  # kasra
    "~": "\u0651",  # shadda
    "o": "\u0652",  # sukun
    "`": "\u0670",  # dagger alif (superscript alif)
    "{": "\u0671",  # alif wasla (hamzat wasl)

    # Extended Buckwalter symbols
    "P": "\u067E",  # peh (Persian)
    "J": "\u0686",  # tcheh (Persian)
    "V": "\u06A4",  # veh (Persian)
    "G": "\u06AF",  # gaf (Persian)
    "R": "\u0695",  # reh with small v below (Kurdish)
    "O": "\u06C1",  # heh goal (Urdu)
    "W": "\u06CF",  # waw with dot above (Kurdish)

    # Additional diacritics
    "^": "\u0653",  # maddah above
    "#": "\u0654",  # hamza above
    "`": "\u0670",  # superscript alif
    "\"": "\u06DF",  # small high rounded zero
    "[": "\u06DC",  # small high seen
    ";": "\u061B",  # Arabic semicolon
    ",": "\u060C",  # Arabic comma
    ".": "\u06D4",  # Arabic full stop
    "?": "\u061F",  # Arabic question mark

    # XML-safe alternatives (for the XML-safe version of Buckwalter)
    "O": "\u0647",  # Alternative mapping for heh (XML-safe)
    "I": "\u0649",  # Alternative mapping for alif maqṣūra (XML-safe)
    "W": "\u0624",  # Alternative mapping for hamza-on-waw (XML-safe)
    "Q": "\u0626",  # Alternative mapping for hamza-on-ya (XML-safe)

    # Additional punctuation and symbols
    "-": "\u0640",  # tatweel (kashida)
    "%": "\u066A",  # Arabic percent sign
    "0": "\u0660",  # Arabic-Indic digit 0
    "1": "\u0661",  # Arabic-Indic digit 1
    "2": "\u0662",  # Arabic-Indic digit 2
    "3": "\u0663",  # Arabic-Indic digit 3
    "4": "\u0664",  # Arabic-Indic digit 4
    "5": "\u0665",  # Arabic-Indic digit 5
    "6": "\u0666",  # Arabic-Indic digit 6
    "7": "\u0667",  # Arabic-Indic digit 7
    "8": "\u0668",  # Arabic-Indic digit 8
    "9": "\u0669",  # Arabic-Indic digit 9
}

def buckwalter_to_arabic(text, preserve_latin=False, preserve_digits=True):
    """
    Convert a Buckwalter-transliterated string to Arabic script.

    Parameters:
        text (str): Input text in Buckwalter transliteration.
        preserve_latin (bool): If True, preserve Latin characters that don't have mappings.
                             If False, keep them unchanged (default False).
        preserve_digits (bool): If True, keep Western (ASCII) digits as is.
                              If False, convert to Arabic-Indic digits (default True).

    Returns:
        str: The corresponding Arabic-script string.
    """
    if not text:
        return ""

    result = []
    i = 0
    while i < len(text):
        ch = text[i]

        # Handle special sequences
        if i < len(text) - 1 and ch + text[i+1] == "lA":  # lam-alif ligature
            result.append("\u0644\u0627")
            i += 2
            continue
        elif i < len(text) - 1 and ch + text[i+1] == "lM":  # lam-alif with madda
            result.append("\u0644\u0622")
            i += 2
            continue

        # Handle regular digits if preserve_digits is True
        if preserve_digits and ch.isdigit():
            result.append(ch)
            i += 1
            continue

        # Handle spaces and punctuation that should be preserved
        if ch.isspace() or ch in '!@#$%^&*()_+-=[]{}|;:",.<>?/':
            if ch not in BUCKWALTER_TO_ARABIC:  # Only preserve if not in the mapping
                result.append(ch)
                i += 1
                continue

        # Standard character mapping
        arabic_char = BUCKWALTER_TO_ARABIC.get(ch)

        # If not found in the mapping
        if arabic_char is None:
            if preserve_latin or not ch.isalpha():
                result.append(ch)  # Keep Latin characters and non-alphabetic characters unchanged
            else:
                # Skip characters without mapping if not preserving Latin
                pass
        else:
            result.append(arabic_char)

        i += 1

    return "".join(result)


def arabic_to_buckwalter(text):
    """
    Convert Arabic script to Buckwalter transliteration.

    Parameters:
        text (str): Input text in Arabic script.

    Returns:
        str: The corresponding Buckwalter transliteration.
    """
    # Create a reverse mapping dictionary
    ARABIC_TO_BUCKWALTER = {v: k for k, v in BUCKWALTER_TO_ARABIC.items()}

    result = []
    for ch in text:
        # Map each character, or leave it unchanged if no mapping exists
        result.append(ARABIC_TO_BUCKWALTER.get(ch, ch))
    return "".join(result)


# Example usage
if __name__ == "__main__":
    # Test the function with some examples
    buck_text = "Al-salAmu Ealaykum"
    arabic_text = buckwalter_to_arabic(buck_text)
    print(f"Original: {buck_text}")
    print(f"Arabic: {arabic_text}")

    # Test round-trip conversion
    back_to_buck = arabic_to_buckwalter(arabic_text)
    print(f"Back to Buckwalter: {back_to_buck}")

"""# preparing tha dataset

all of the datasets have samiliar distribution as the records are 5-20 seconds and for each split 200 records which satfies the minimum amount of data
"""

from datasets import load_dataset, concatenate_datasets, Audio, Dataset
import random

# 1️⃣ Load raw “train” splits
egy_raw = load_dataset("MightyStudent/Egyptian-ASR-MGB-3", split="train")
cl_raw  = load_dataset("MBZUAI/ClArTTS", split="train")
msa_raw = load_dataset("halabi2016/arabic_speech_corpus", split="train")

# 2️⃣ Keep only 'audio' and 'text'
def keep_columns(ds, text_col):
    ds = ds.rename_column(text_col, "text")
    return ds.remove_columns([c for c in ds.column_names if c not in {"audio", "text"}])

egy = keep_columns(egy_raw, "sentence")
msa = msa_raw.remove_columns([c for c in msa_raw.column_names if c not in {"audio", "text"}])

# Subsample 1000 from ClArTTS up front
cl_raw_subset = cl_raw.shuffle(seed=0).select(range(1000))

def wrap_classic(ex):
    return {
        "text":  ex["text"],
        "audio": {"array": ex["audio"], "sampling_rate": ex["sampling_rate"]}
    }

cl = cl_raw_subset.map(wrap_classic, remove_columns=[c for c in cl_raw_subset.column_names if c not in {"audio", "text"}])

# 3️⃣ Cast all audio to 16kHz
egy = egy.cast_column("audio", Audio(sampling_rate=16000))
cl  = cl.cast_column("audio", Audio(sampling_rate=16000))
msa = msa.cast_column("audio", Audio(sampling_rate=16000))

# 4️⃣ Select examples until each dataset has at least 30 minutes (~1800 sec)
def select_until_duration(dataset, min_seconds):
    total, selected = 0.0, []
    for ex in dataset:
        array = ex["audio"]["array"]
        sr = ex["audio"]["sampling_rate"]
        duration = len(array) / sr
        ex["duration"] = duration
        total += duration
        selected.append(ex)
        if total >= min_seconds:
            break
    return selected

target_duration = 1800  # 30 minutes in seconds

# Shuffle before selection
egy = egy.shuffle(seed=1)
msa = msa.shuffle(seed=1)
cl  = cl.shuffle(seed=1)

# Select just enough from each
egy_sel = select_until_duration(egy, target_duration)
msa_sel = select_until_duration(msa, target_duration)
cl_sel  = select_until_duration(cl,  target_duration)

# Convert back to datasets
egy_bal = Dataset.from_list(egy_sel)
msa_bal = Dataset.from_list(msa_sel)
cl_bal  = Dataset.from_list(cl_sel)



msa_bal = msa_bal.map(lambda ex, idx: {"text": buckwalter_to_arabic(ex["text"])} , with_indices=True)

# 6️⃣ Combine all datasets
combined = concatenate_datasets([egy_bal, msa_bal, cl_bal])
print(combined)

# 7️⃣ Print durations for each
def compute_duration_stats(dataset, name=""):
    durations = dataset["duration"]
    total_duration = sum(durations)
    avg_duration = total_duration / len(durations)
    print(f"{name} — Total: {total_duration:.2f}s, Avg/sample: {avg_duration:.2f}s")

compute_duration_stats(egy_bal, "Egyptian Arabic")
compute_duration_stats(msa_bal, "MSA")
compute_duration_stats(cl_bal,  "Classical Arabic")

# 8️⃣ Optionally save the combined dataset
combined.save_to_disk("combined_dataset_balanced")

from google.colab import drive
drive.mount('/content/drive')

!cp -r combined_dataset_balanced /content/drive/MyDrive/

!cp -r /content/drive/MyDrive/combined_dataset_balanced /content/

from datasets import load_from_disk

combined = load_from_disk("combined_dataset_balanced")

!pip install jiwer

import torch
from transformers import pipeline, AutoProcessor, AutoModelForSpeechSeq2Seq
from jiwer import wer
import librosa
import evaluate

device = "cuda:0" if torch.cuda.is_available() else "cpu"

"""## wav2vec2-large-xlsr-53-arabic model"""

# Initialize the ASR pipeline
pipe_wav2vec2 = pipeline("automatic-speech-recognition", model="jonatasgrosman/wav2vec2-large-xlsr-53-arabic")

def resample_audio(audio_array, orig_sr, target_sr=16000):
    if orig_sr != target_sr:
        return librosa.resample(audio_array, orig_sr=orig_sr, target_sr=target_sr)
    return audio_array

import numpy as np

# Ensure that each audio is a NumPy array, as the pipeline expects ndarray
audios = [np.array(ex["audio"]["array"]) for ex in combined]

# Transcribe in batch
transcriptions = pipe_wav2vec2(audios, batch_size=16)
predicted_texts = [t["text"] for t in transcriptions]


# Extract ground truth texts
ground_truth = combined["text"]

cer = evaluate.load("cer")
cer_score_wav2vec2 = cer.compute(predictions=predicted_texts, references=ground_truth)
print(f"CER: {cer_score_wav2vec2}")

"""## wav2vec2-large-xlsr-53-arabic-egyptian"""

# Use a pipeline as a high-level helper
from transformers import pipeline

pipe_wav2vec2_egy = pipeline("automatic-speech-recognition", model="arbml/wav2vec2-large-xlsr-53-arabic-egyptian")

def resample_audio(audio_array, orig_sr, target_sr=16000):
    if orig_sr != target_sr:
        return librosa.resample(audio_array, orig_sr=orig_sr, target_sr=target_sr)
    return audio_array

import numpy as np

# Ensure that each audio is a NumPy array, as the pipeline expects ndarray
audios = [np.array(ex["audio"]["array"]) for ex in combined]

# Transcribe in batch
transcriptions = pipe_wav2vec2_egy(audios, batch_size=16)
predicted_texts = [t["text"] for t in transcriptions]


# Extract ground truth texts
ground_truth = combined["text"]

cer = evaluate.load("cer")
cer_score_wav2vec2_egy = cer.compute(predictions=predicted_texts, references=ground_truth)
print(f"CER: {cer_score_wav2vec2_egy}")

"""## whisper-large-v3

"""

from transformers import pipeline

pipe_whisper = pipeline(
    "automatic-speech-recognition",
    model="openai/whisper-large-v3",
    device=device,
    chunk_length_s=30,
    stride_length_s=(5, 5)
)

def resample_audio(audio_array, orig_sr, target_sr=16000):
    if orig_sr != target_sr:
        return librosa.resample(audio_array, orig_sr=orig_sr, target_sr=target_sr)
    return audio_array

import numpy as np

# Ensure that each audio is a NumPy array, as the pipeline expects ndarray
audios = [np.array(ex["audio"]["array"]) for ex in combined]

# Run inference
out_whisper = pipe_whisper(audios)
hyp_whisper = [x["text"] for x in out_whisper]

cer = evaluate.load("cer")
cer_score_whisper3 = cer.compute(predictions=hyp_whisper, references=ground_truth)
print(f"CER: {cer_score_whisper3}")

"""## Whisper Small-Ar"""

# Whisper Small-Ar
pipe_s = pipeline(
    "automatic-speech-recognition", model="ayoubkirouane/whisper-small-ar",
    chunk_length_s=30, stride_length_s=(5,5), batch_size=8,
    generate_kwargs={"language":"arabic","task":"transcribe"},
    device=device
)

def resample_audio(audio_array, orig_sr, target_sr=16000):
    if orig_sr != target_sr:
        return librosa.resample(audio_array, orig_sr=orig_sr, target_sr=target_sr)
    return audio_array

import numpy as np

# Ensure that each audio is a NumPy array, as the pipeline expects ndarray
audios = [np.array(ex["audio"]["array"]) for ex in combined]

print("Running Whisper-Small-Ar...")
out_s = pipe_s(audios)
hyp_s = [x['text'] for x in out_s]

cer = evaluate.load("cer")
cer_score_s = cer.compute(predictions=hyp_s, references=ground_truth)
print(f"CER: {cer_score_s}")

"""## HuBERT Egyptian CTC"""

from transformers import HubertForCTC, Wav2Vec2Processor

# feature‐extractor / tokenizer stays the same
wv_proc  = Wav2Vec2Processor.from_pretrained("omarxadel/hubert-large-arabic-egyptian")

# load with HubertForCTC, not Wav2Vec2ForCTC
wv_model = HubertForCTC.from_pretrained("omarxadel/hubert-large-arabic-egyptian").to(device)

def resample_audio(audio_array, orig_sr, target_sr=16000):
    if orig_sr != target_sr:
        return librosa.resample(audio_array, orig_sr=orig_sr, target_sr=target_sr)
    return audio_array

import numpy as np
import math

# Ensure that each audio is a NumPy array, as the pipeline expects ndarray
audios = [np.array(ex["audio"]["array"]) for ex in combined]

all_preds = []
BATCH = 16
audio_arrays = [np.array(ex["audio"]["array"]) for ex in combined]

for i in range(math.ceil(len(audio_arrays)/BATCH)):
    chunk = audio_arrays[i*BATCH:(i+1)*BATCH]
    wv_inp = wv_proc(chunk, sampling_rate=16000, return_tensors="pt", padding=True)
    wv_inp = {k: v.to(device) for k,v in wv_inp.items()}
    with torch.no_grad():
        logits = wv_model(wv_inp["input_values"], attention_mask=wv_inp["attention_mask"]).logits
    pred_ids = torch.argmax(logits, dim=-1)
    all_preds += wv_proc.batch_decode(pred_ids)
    print(f"Completed batch {i+1}/{math.ceil(len(audio_arrays)/BATCH)}")

ground_truth   = combined["text"]

cer = evaluate.load("cer")
cer_score_wv = cer.compute(predictions=all_preds, references=ground_truth)
print(f"CER: {cer_score_wv}")

""" ## Team‑ASR‑2023/wav2vec2‑large‑xls‑r‑300m‑Arabic‑colab"""

from transformers import pipeline

pipe_Team_ASR = pipeline("automatic-speech-recognition", model="Team-ASR-2023/wav2vec2-large-xls-r-300m-Arabic")

def resample_audio(audio_array, orig_sr, target_sr=16000):
    if orig_sr != target_sr:
        return librosa.resample(audio_array, orig_sr=orig_sr, target_sr=target_sr)
    return audio_array

import numpy as np

# Ensure that each audio is a NumPy array, as the pipeline expects ndarray
audios = [np.array(ex["audio"]["array"]) for ex in combined]

# Run inference
out_Team_ASR= pipe_Team_ASR(audios)

hyp_Team_ASR = [x["text"] for x in out_Team_ASR]
cer = evaluate.load("cer")
cer_Team_ASR = cer.compute(predictions=hyp_Team_ASR, references=ground_truth)
print(f"CER: {cer_Team_ASR}")