{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eVXa3vhTj_sx",
        "outputId": "21b3ae0a-cc8a-4f04-827a-79f272e29745"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting pytesseract\n",
            "  Downloading pytesseract-0.3.13-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting pdf2image\n",
            "  Downloading pdf2image-1.17.0-py3-none-any.whl.metadata (6.2 kB)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (11.2.1)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.11/dist-packages (from pytesseract) (24.2)\n",
            "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/232.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytesseract-0.3.13-py3-none-any.whl (14 kB)\n",
            "Downloading pdf2image-1.17.0-py3-none-any.whl (11 kB)\n",
            "Installing collected packages: pytesseract, PyPDF2, pdf2image\n",
            "Successfully installed PyPDF2-3.0.1 pdf2image-1.17.0 pytesseract-0.3.13\n"
          ]
        }
      ],
      "source": [
        "!pip install PyPDF2 pytesseract pdf2image pillow"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt-get install tesseract-ocr tesseract-ocr-ara"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IK6rF24ty0-d",
        "outputId": "ec49638a-01df-495e-9521-3b2b51be4714"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "tesseract-ocr is already the newest version (4.1.1-2.1build1).\n",
            "The following NEW packages will be installed:\n",
            "  tesseract-ocr-ara\n",
            "0 upgraded, 1 newly installed, 0 to remove and 34 not upgraded.\n",
            "Need to get 645 kB of archives.\n",
            "After this operation, 1,447 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-ara all 1:4.00~git30-7274cfa-1.1 [645 kB]\n",
            "Fetched 645 kB in 1s (491 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 1.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package tesseract-ocr-ara.\n",
            "(Reading database ... 126101 files and directories currently installed.)\n",
            "Preparing to unpack .../tesseract-ocr-ara_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-ara (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-ara (1:4.00~git30-7274cfa-1.1) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages\n",
        "!pip install PyPDF2 pytesseract pdf2image\n",
        "\n",
        "# Download and install tesseract + Arabic language pack if running in Colab\n",
        "import os\n",
        "if 'google.colab' in str(get_ipython()):\n",
        "    !apt-get install tesseract-ocr\n",
        "    !apt-get install tesseract-ocr-ara\n",
        "    !apt-get install poppler-utils\n",
        "\n",
        "import re\n",
        "import json\n",
        "import PyPDF2\n",
        "import pytesseract\n",
        "from pdf2image import convert_from_path\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import tempfile\n",
        "from google.colab import files"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zy02gL4WPYC_",
        "outputId": "a20f7127-aa3d-440a-dd0c-ab3c4b5d4bd5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.11/dist-packages (3.0.1)\n",
            "Requirement already satisfied: pytesseract in /usr/local/lib/python3.11/dist-packages (0.3.13)\n",
            "Requirement already satisfied: pdf2image in /usr/local/lib/python3.11/dist-packages (1.17.0)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.11/dist-packages (from pytesseract) (24.2)\n",
            "Requirement already satisfied: Pillow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from pytesseract) (11.2.1)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "tesseract-ocr is already the newest version (4.1.1-2.1build1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 34 not upgraded.\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "tesseract-ocr-ara is already the newest version (1:4.00~git30-7274cfa-1.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 34 not upgraded.\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  poppler-utils\n",
            "0 upgraded, 1 newly installed, 0 to remove and 34 not upgraded.\n",
            "Need to get 186 kB of archives.\n",
            "After this operation, 696 kB of additional disk space will be used.\n",
            "Ign:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 poppler-utils amd64 22.02.0-2ubuntu0.7\n",
            "Err:1 http://security.ubuntu.com/ubuntu jammy-updates/main amd64 poppler-utils amd64 22.02.0-2ubuntu0.7\n",
            "  404  Not Found [IP: 185.125.190.83 80]\n",
            "E: Failed to fetch http://security.ubuntu.com/ubuntu/pool/main/p/poppler/poppler-utils_22.02.0-2ubuntu0.7_amd64.deb  404  Not Found [IP: 185.125.190.83 80]\n",
            "E: Unable to fetch some archives, maybe run apt-get update or try with --fix-missing?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import json\n",
        "from google.colab import files\n",
        "import PyPDF2\n",
        "from pdf2image import convert_from_path\n",
        "import pytesseract\n",
        "\n",
        "def extract_text_from_pdf(pdf_path, use_ocr=True):\n",
        "    \"\"\"\n",
        "    Extract text from a PDF file.\n",
        "    If use_ocr is True, uses OCR (recommended for Arabic PDFs).\n",
        "    Otherwise, tries to extract text directly (might not work well with Arabic).\n",
        "    \"\"\"\n",
        "    if not use_ocr:\n",
        "        # Try to extract text directly from PDF\n",
        "        text = \"\"\n",
        "        with open(pdf_path, 'rb') as file:\n",
        "            pdf_reader = PyPDF2.PdfReader(file)\n",
        "            for page_num in range(len(pdf_reader.pages)):\n",
        "                page = pdf_reader.pages[page_num]\n",
        "                text += page.extract_text() + \"\\n\\n\"\n",
        "        return text\n",
        "\n",
        "    # If OCR is requested or direct extraction didn't yield good results\n",
        "    print(\"Using OCR to extract text from PDF...\")\n",
        "\n",
        "    # Convert PDF to images\n",
        "    print(\"Converting PDF to images...\")\n",
        "    images = convert_from_path(pdf_path)\n",
        "\n",
        "    # Extract text from each image using OCR with Arabic language\n",
        "    full_text = \"\"\n",
        "    for i, image in enumerate(images):\n",
        "        print(f\"Processing page {i+1}/{len(images)}...\")\n",
        "        text = pytesseract.image_to_string(image, lang='ara')  # 'ara' for Arabic\n",
        "        full_text += text + \"\\n\\n\"\n",
        "\n",
        "    return full_text\n",
        "\n",
        "def split_into_paragraphs(text):\n",
        "    \"\"\"Split text into paragraphs based on new lines.\"\"\"\n",
        "    paragraphs = re.split(r'\\n+', text)\n",
        "    return [p.strip() for p in paragraphs if p.strip()]\n",
        "\n",
        "def split_into_sentences(paragraph):\n",
        "    \"\"\"Split an Arabic paragraph into sentences.\"\"\"\n",
        "    sentences = re.split(r'([.!?؟।\\n]+)', paragraph)\n",
        "    processed_sentences = []\n",
        "    for i in range(0, len(sentences)-1, 2):\n",
        "        if i+1 < len(sentences):\n",
        "            processed_sentences.append(sentences[i] + sentences[i+1])\n",
        "        else:\n",
        "            processed_sentences.append(sentences[i])\n",
        "    if len(sentences) % 2 == 1 and sentences[-1].strip():\n",
        "        processed_sentences.append(sentences[-1])\n",
        "    return [s.strip() for s in processed_sentences if s.strip()]\n",
        "\n",
        "def chunk_paragraphs(paragraphs, min_sentences=2, max_sentences=4):\n",
        "    \"\"\"\n",
        "    Split paragraphs into chunks of 2-4 sentences.\n",
        "    Returns a list of chunks.\n",
        "    \"\"\"\n",
        "    chunks = []\n",
        "    for paragraph in paragraphs:\n",
        "        sentences = split_into_sentences(paragraph)\n",
        "        if len(sentences) <= min_sentences:\n",
        "            chunks.append(\" \".join(sentences))\n",
        "            continue\n",
        "        current_chunk = []\n",
        "        for sentence in sentences:\n",
        "            current_chunk.append(sentence)\n",
        "            if len(current_chunk) >= max_sentences:\n",
        "                chunks.append(\" \".join(current_chunk))\n",
        "                current_chunk = []\n",
        "        if current_chunk:\n",
        "            chunks.append(\" \".join(current_chunk))\n",
        "    return chunks\n",
        "\n",
        "def process_arabic_pdf(pdf_path, output_file='chunked_arabic_book.json', use_ocr=True):\n",
        "    \"\"\"Process an Arabic PDF and save chunks to a JSON file.\"\"\"\n",
        "    text = extract_text_from_pdf(pdf_path, use_ocr)\n",
        "    text_file = os.path.splitext(output_file)[0] + \"_full_text.txt\"\n",
        "    with open(text_file, 'w', encoding='utf-8') as f:\n",
        "        f.write(text)\n",
        "    print(f\"Full extracted text saved to {text_file}\")\n",
        "    paragraphs = split_into_paragraphs(text)\n",
        "    chunks = chunk_paragraphs(paragraphs)\n",
        "    output_data = {\n",
        "        \"source\": os.path.basename(pdf_path),\n",
        "        \"chunks\": [{\"id\": i, \"text\": chunk} for i, chunk in enumerate(chunks)]\n",
        "    }\n",
        "    with open(output_file, 'w', encoding='utf-8') as f:\n",
        "        json.dump(output_data, f, ensure_ascii=False, indent=2)\n",
        "    print(f\"Processed {len(paragraphs)} paragraphs into {len(chunks)} chunks\")\n",
        "    print(f\"Output saved to {output_file}\")\n",
        "    return chunks, text_file, output_file\n",
        "\n",
        "# For Google Colab: Check if running in Colab and handle file input\n",
        "try:\n",
        "    from google.colab import files\n",
        "    print(\"Running in Google Colab environment\")\n",
        "    print(\"Would you like to:\")\n",
        "    print(\"1. Upload the Arabic PDF file\")\n",
        "    print(\"2. Enter the file path to the Arabic PDF file\")\n",
        "    choice = input(\"Enter your choice (1 or 2): \").strip()\n",
        "\n",
        "    if choice == '1':\n",
        "        print(\"Please upload your Arabic PDF file...\")\n",
        "        uploaded = files.upload()\n",
        "        pdf_filename = list(uploaded.keys())[0]\n",
        "    elif choice == '2':\n",
        "        pdf_filename = input(\"Enter the path to your Arabic PDF file (e.g., /content/book.pdf): \").strip()\n",
        "        if not pdf_filename:\n",
        "            raise ValueError(\"File path cannot be empty\")\n",
        "    else:\n",
        "        raise ValueError(\"Invalid choice. Please select 1 or 2.\")\n",
        "\n",
        "    # Ask about OCR\n",
        "    print(\"\\nWould you like to use OCR for text extraction?\")\n",
        "    print(\"1. Yes (recommended for Arabic PDFs)\")\n",
        "    print(\"2. No (direct text extraction, may not work well for Arabic)\")\n",
        "    ocr_choice = input(\"Enter your choice (1 or 2): \").strip()\n",
        "    use_ocr = True if ocr_choice == '1' else False\n",
        "\n",
        "    # Process the PDF\n",
        "    print(f\"Processing {pdf_filename}...\")\n",
        "    chunks, text_file, json_file = process_arabic_pdf(pdf_filename, use_ocr=use_ocr)\n",
        "\n",
        "    # Display a sample of chunks\n",
        "    print(\"\\nSample chunks:\")\n",
        "    for i, chunk in enumerate(chunks[:3]):\n",
        "        print(f\"Chunk {i}: {chunk[:100]}...\")\n",
        "\n",
        "    # Download the output files\n",
        "    print(\"\\nDownloading output files...\")\n",
        "    files.download(text_file)\n",
        "    files.download(json_file)\n",
        "\n",
        "except ImportError:\n",
        "    # Not running in Colab\n",
        "    print(\"Not running in Colab environment\")\n",
        "    pdf_filename = input(\"Enter the path to your Arabic PDF file: \").strip()\n",
        "    if not pdf_filename:\n",
        "        raise ValueError(\"File path cannot be empty\")\n",
        "\n",
        "    # Ask about OCR\n",
        "    print(\"\\nWould you like to use OCR for text extraction?\")\n",
        "    print(\"1. Yes (recommended for Arabic PDFs)\")\n",
        "    print(\"2. No (direct text extraction, may not work well for Arabic)\")\n",
        "    ocr_choice = input(\"Enter your choice (1 or 2): \").strip()\n",
        "    use_ocr = True if ocr_choice == '1' else False\n",
        "\n",
        "    # Process the PDF\n",
        "    print(f\"Processing {pdf_filename}...\")\n",
        "    chunks, text_file, json_file = process_arabic_pdf(pdf_filename, use_ocr=use_ocr)\n",
        "\n",
        "    # Display a sample of chunks\n",
        "    print(\"\\nSample chunks:\")\n",
        "    for i, chunk in enumerate(chunks[:3]):\n",
        "        print(f\"Chunk {i}: {chunk[:100]}...\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 271
        },
        "id": "aollMY8fy5Ve",
        "outputId": "2d5b4acf-5084-47d8-b8cc-f0dcfa3c65e3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please upload your Arabic PDF file...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-5870f1bd-d50b-486a-b657-8f53822db59c\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-5870f1bd-d50b-486a-b657-8f53822db59c\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving law.pdf to law.pdf\n",
            "Processing law.pdf...\n",
            "Full extracted text saved to chunked_arabic_book_full_text.txt\n",
            "Processed 1212 paragraphs into 1214 chunks\n",
            "Output saved to chunked_arabic_book.json\n",
            "\n",
            "Sample chunks:\n",
            "Chunk 0: دستور مصر بعد تعديالت2019...\n",
            "Chunk 1: ملخص التعديالت...\n",
            "Chunk 2: في يوم ٣٢ أبريل ٩١٠٢ أعلنت الهيئة الوطنية لألنتخابات نتائج االستفتاء علي تعديل بعض مواد الدستور والذ...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_c173c6ec-a920-4c9a-832f-ae7492949a1d\", \"chunked_arabic_book_full_text.txt\", 165099)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_e2283218-b1b3-4d80-9d57-934932e0d42b\", \"chunked_arabic_book.json\", 218976)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence-transformers transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QnZ3W-PMzRwJ",
        "outputId": "3d6c122b-41d5-4f2a-9b1f-d4d4552b68de"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (3.4.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.6.0+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.15.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.30.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.2.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m98.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m71.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m43.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m837.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m43.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import torch\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from tqdm.notebook import tqdm  # For progress tracking in notebooks\n",
        "\n",
        "# Load the model\n",
        "model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n",
        "\n",
        "# Function to generate embeddings\n",
        "def generate_embeddings(texts):\n",
        "    return model.encode(texts, convert_to_tensor=True)\n",
        "\n",
        "# Load chunks from the JSON file\n",
        "def load_chunks(json_file_path):\n",
        "    with open(json_file_path, 'r', encoding='utf-8') as f:\n",
        "        data = json.load(f)\n",
        "    return data\n",
        "\n",
        "# Process and save embeddings\n",
        "def process_chunks_with_embeddings(json_file_path, output_path='chunks_with_embeddings.json'):\n",
        "    # Load data\n",
        "    data = load_chunks(json_file_path)\n",
        "\n",
        "    # Extract texts from chunks\n",
        "    chunk_texts = [chunk['text'] for chunk in data['chunks']]\n",
        "    print(f\"Generating embeddings for {len(chunk_texts)} chunks...\")\n",
        "\n",
        "    # Generate embeddings\n",
        "    embeddings = generate_embeddings(chunk_texts)\n",
        "    print(f\"Shape of embeddings: {embeddings.shape}\")\n",
        "\n",
        "    # Convert to numpy for saving\n",
        "    embeddings_numpy = embeddings.cpu().numpy()\n",
        "\n",
        "    # Add embeddings to chunks\n",
        "    for i, embedding in enumerate(embeddings_numpy):\n",
        "        data['chunks'][i]['embedding'] = embedding.tolist()\n",
        "\n",
        "    # Save the data with embeddings\n",
        "    with open(output_path, 'w', encoding='utf-8') as f:\n",
        "        json.dump(data, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    # Also save embeddings separately as numpy array\n",
        "    np.save('chunk_embeddings.npy', embeddings_numpy)\n",
        "\n",
        "    print(f\"Saved chunks with embeddings to {output_path}\")\n",
        "    print(f\"Saved raw embeddings to chunk_embeddings.npy\")\n",
        "\n",
        "    return data, embeddings\n",
        "\n",
        "try:\n",
        "    from google.colab import files\n",
        "    print(\"Running in Google Colab environment\")\n",
        "    print(\"Would you like to:\")\n",
        "    print(\"1. Upload the chunked JSON file\")\n",
        "    print(\"2. Enter the file path to the chunked JSON file\")\n",
        "    choice = input(\"Enter your choice (1 or 2): \").strip()\n",
        "\n",
        "    if choice == '1':\n",
        "        print(\"Please upload your chunked Arabic book JSON file...\")\n",
        "        uploaded = files.upload()\n",
        "        json_file = list(uploaded.keys())[0]\n",
        "    elif choice == '2':\n",
        "        json_file = input(\"Enter the path to your chunked JSON file (e.g., /content/chunked_arabic_book.json): \").strip()\n",
        "        if not json_file:\n",
        "            raise ValueError(\"File path cannot be empty\")\n",
        "    else:\n",
        "        raise ValueError(\"Invalid choice. Please select 1 or 2.\")\n",
        "\n",
        "    # Process the data\n",
        "    data, embeddings = process_chunks_with_embeddings(json_file)\n",
        "\n",
        "    # Download the results\n",
        "    print(\"Downloading the files with embeddings...\")\n",
        "    files.download('chunks_with_embeddings.json')\n",
        "    files.download('chunk_embeddings.npy')\n",
        "\n",
        "except ImportError:\n",
        "    # Not running in Colab\n",
        "    print(\"Not running in Colab environment\")\n",
        "    json_file = input(\"Enter the path to your chunked JSON file: \").strip()\n",
        "    if not json_file:\n",
        "        raise ValueError(\"File path cannot be empty\")\n",
        "\n",
        "    # Process the data\n",
        "    data, embeddings = process_chunks_with_embeddings(json_file)\n",
        "\n",
        "# Show the first embedding as a sample\n",
        "print(\"\\nSample embedding (first 5 dimensions):\")\n",
        "print(embeddings[0][:5])\n",
        "print(f\"Total embedding dimensions: {embeddings.shape[1]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 450
        },
        "id": "hnKJjO3U6dLT",
        "outputId": "80ec5ff2-5f37-4543-e1be-c51173d50ab4"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running in Google Colab environment\n",
            "Would you like to:\n",
            "1. Upload the chunked JSON file\n",
            "2. Enter the file path to the chunked JSON file\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-2b5560f7b986>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"1. Upload the chunked JSON file\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"2. Enter the file path to the chunked JSON file\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m     \u001b[0mchoice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Enter your choice (1 or 2): \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchoice\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'1'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1175\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m             )\n\u001b[0;32m-> 1177\u001b[0;31m         return self._input_request(\n\u001b[0m\u001b[1;32m   1178\u001b[0m             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shell\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1219\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1220\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faiss-cpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K8OFBl_VDYh1",
        "outputId": "8a66404b-428e-43fe-cb57-dca4055e35e7"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.11.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (4.8 kB)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\n",
            "Downloading faiss_cpu-1.11.0-cp311-cp311-manylinux_2_28_x86_64.whl (31.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m48.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.11.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip list | grep faiss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ALlpB4hBDzQa",
        "outputId": "bad3ff09-510e-47b4-d1e8-964a3894009c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "faiss-cpu                             1.11.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import faiss\n",
        "import os\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "def load_embeddings(embeddings_file='chunk_embeddings.npy'):\n",
        "    \"\"\"Load the embeddings from the NumPy file.\"\"\"\n",
        "    if os.path.exists(embeddings_file):\n",
        "        embeddings = np.load(embeddings_file)\n",
        "        print(f\"Loaded embeddings with shape: {embeddings.shape}\")\n",
        "        return embeddings\n",
        "    else:\n",
        "        raise FileNotFoundError(f\"Embeddings file {embeddings_file} not found\")\n",
        "\n",
        "def load_chunks_with_embeddings(json_file='chunks_with_embeddings.json'):\n",
        "    \"\"\"Load chunks with their embeddings from the JSON file.\"\"\"\n",
        "    with open(json_file, 'r', encoding='utf-8') as f:\n",
        "        data = json.load(f)\n",
        "    print(f\"Loaded {len(data['chunks'])} chunks with embeddings\")\n",
        "    return data\n",
        "\n",
        "def create_faiss_index(embeddings, index_type='flat'):\n",
        "    \"\"\"\n",
        "    Create a FAISS index from the embeddings.\n",
        "\n",
        "    Parameters:\n",
        "    - embeddings: numpy array of embeddings\n",
        "    - index_type: type of FAISS index to create ('flat', 'ivf', or 'hnsw')\n",
        "\n",
        "    Returns:\n",
        "    - faiss index\n",
        "    \"\"\"\n",
        "    vector_dimension = embeddings.shape[1]\n",
        "    num_vectors = embeddings.shape[0]\n",
        "\n",
        "    print(f\"Creating FAISS index for {num_vectors} vectors with {vector_dimension} dimensions\")\n",
        "\n",
        "    embeddings_normalized = embeddings.copy()\n",
        "    faiss.normalize_L2(embeddings_normalized)\n",
        "\n",
        "    if index_type == 'flat':\n",
        "        index = faiss.IndexFlatIP(vector_dimension)\n",
        "        index.add(embeddings_normalized)\n",
        "\n",
        "    elif index_type == 'ivf':\n",
        "        nlist = int(np.sqrt(num_vectors))\n",
        "        quantizer = faiss.IndexFlatIP(vector_dimension)\n",
        "        index = faiss.IndexIVFFlat(quantizer, vector_dimension, nlist, faiss.METRIC_INNER_PRODUCT)\n",
        "        print(\"Training IVF index...\")\n",
        "        index.train(embeddings_normalized)\n",
        "        index.add(embeddings_normalized)\n",
        "        index.nprobe = min(10, nlist)\n",
        "\n",
        "    elif index_type == 'hnsw':\n",
        "        M = 16\n",
        "        index = faiss.IndexHNSWFlat(vector_dimension, M, faiss.METRIC_INNER_PRODUCT)\n",
        "        index.add(embeddings_normalized)\n",
        "\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown index type: {index_type}\")\n",
        "\n",
        "    print(f\"Created {index_type.upper()} index with {index.ntotal} vectors\")\n",
        "    return index\n",
        "\n",
        "def save_faiss_index(index, output_file='arabic_embeddings.faiss'):\n",
        "    \"\"\"Save the FAISS index to a file.\"\"\"\n",
        "    faiss.write_index(index, output_file)\n",
        "    print(f\"Saved FAISS index to {output_file}\")\n",
        "    return output_file\n",
        "\n",
        "def test_faiss_index(index, embeddings, chunks_data, query_text=\"مرحبا\", top_k=5):\n",
        "    \"\"\"\n",
        "    Test the FAISS index with a query and display results.\n",
        "\n",
        "    Parameters:\n",
        "    - index: FAISS index\n",
        "    - embeddings: original embeddings\n",
        "    - chunks_data: original chunks with text\n",
        "    - query_text: Arabic query text\n",
        "    - top_k: number of results to return\n",
        "    \"\"\"\n",
        "    print(f\"\\nTesting index with query: '{query_text}'\")\n",
        "\n",
        "    model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n",
        "\n",
        "    query_embedding = model.encode([query_text], convert_to_tensor=True)\n",
        "    query_embedding_np = query_embedding.cpu().numpy()\n",
        "\n",
        "    faiss.normalize_L2(query_embedding_np)\n",
        "\n",
        "    distances, indices = index.search(query_embedding_np, top_k)\n",
        "\n",
        "    print(f\"\\nTop {top_k} results:\")\n",
        "    for i, (idx, distance) in enumerate(zip(indices[0], distances[0])):\n",
        "        chunk_text = chunks_data['chunks'][idx]['text']\n",
        "        if len(chunk_text) > 100:\n",
        "            chunk_text = chunk_text[:100] + \"...\"\n",
        "        print(f\"{i+1}. [Score: {distance:.4f}] {chunk_text}\")\n",
        "\n",
        "    return distances, indices\n",
        "\n",
        "def main(embeddings_file='chunk_embeddings.npy',\n",
        "         chunks_file='chunks_with_embeddings.json',\n",
        "         index_type='flat',\n",
        "         output_file='arabic_embeddings.faiss'):\n",
        "    \"\"\"Main function to run the indexing process.\"\"\"\n",
        "    embeddings = load_embeddings(embeddings_file)\n",
        "    chunks_data = load_chunks_with_embeddings(chunks_file)\n",
        "    index = create_faiss_index(embeddings, index_type)\n",
        "    save_faiss_index(index, output_file)\n",
        "    test_query = \"مرحبا\"\n",
        "    test_faiss_index(index, embeddings, chunks_data, test_query)\n",
        "    print(\"\\nFAISS indexing complete!\")\n",
        "    return index, chunks_data\n",
        "\n",
        "# For Google Colab: Check if running in Colab and handle file input\n",
        "try:\n",
        "    from google.colab import files\n",
        "    RUNNING_IN_COLAB = True\n",
        "    print(\"Running in Google Colab environment\")\n",
        "\n",
        "    print(\"Would you like to:\")\n",
        "    print(\"1. Upload the embeddings and chunks files\")\n",
        "    print(\"2. Enter the file paths to the embeddings and chunks files\")\n",
        "    choice = input(\"Enter your choice (1 or 2): \").strip()\n",
        "\n",
        "    if choice == '1':\n",
        "        print(\"Please upload chunk_embeddings.npy and chunks_with_embeddings.json...\")\n",
        "        uploaded = files.upload()\n",
        "        if 'chunk_embeddings.npy' not in uploaded or 'chunks_with_embeddings.json' not in uploaded:\n",
        "            raise ValueError(\"Both chunk_embeddings.npy and chunks_with_embeddings.json must be uploaded\")\n",
        "        embeddings_file = 'chunk_embeddings.npy'\n",
        "        chunks_file = 'chunks_with_embeddings.json'\n",
        "    elif choice == '2':\n",
        "        embeddings_file = input(\"Enter path to embeddings file (e.g., /content/chunk_embeddings.npy): \").strip() or \"chunk_embeddings.npy\"\n",
        "        chunks_file = input(\"Enter path to chunks file (e.g., /content/chunks_with_embeddings.json): \").strip() or \"chunks_with_embeddings.json\"\n",
        "        if not embeddings_file or not chunks_file:\n",
        "            raise ValueError(\"File paths cannot be empty\")\n",
        "    else:\n",
        "        raise ValueError(\"Invalid choice. Please select 1 or 2.\")\n",
        "\n",
        "    # Choose index type\n",
        "    print(\"\\nChoose FAISS index type:\")\n",
        "    print(\"1. Flat index (exact search, slower but most accurate)\")\n",
        "    print(\"2. IVF index (approximate search, good balance of speed and accuracy)\")\n",
        "    print(\"3. HNSW index (very fast, good accuracy, best for large datasets)\")\n",
        "    choice = input(\"Enter your choice (1/2/3): \").strip()\n",
        "\n",
        "    index_type = {\n",
        "        '1': 'flat',\n",
        "        '2': 'ivf',\n",
        "        '3': 'hnsw'\n",
        "    }.get(choice, 'flat')\n",
        "\n",
        "    # Run indexing\n",
        "    index, chunks_data = main(embeddings_file, chunks_file, index_type)\n",
        "\n",
        "    # Download the index\n",
        "    print(\"Downloading the FAISS index file...\")\n",
        "    files.download('arabic_embeddings.faiss')\n",
        "\n",
        "except ImportError:\n",
        "    RUNNING_IN_COLAB = False\n",
        "    print(\"Not running in Colab environment\")\n",
        "\n",
        "    embeddings_file = input(\"Enter path to embeddings file (default: chunk_embeddings.npy): \").strip() or \"chunk_embeddings.npy\"\n",
        "    chunks_file = input(\"Enter path to chunks file (default: chunks_with_embeddings.json): \").strip() or \"chunks_with_embeddings.json\"\n",
        "    if not embeddings_file or not chunks_file:\n",
        "        raise ValueError(\"File paths cannot be empty\")\n",
        "\n",
        "    print(\"\\nChoose FAISS index type:\")\n",
        "    print(\"1. Flat index (exact search, slower but most accurate)\")\n",
        "    print(\"2. IVF index (approximate search, good balance of speed and accuracy)\")\n",
        "    print(\"3. HNSW index (very fast, good accuracy, best for large datasets)\")\n",
        "    choice = input(\"Enter your choice (1/2/3): \").strip()\n",
        "\n",
        "    index_type = {\n",
        "        '1': 'flat',\n",
        "        '2': 'ivf',\n",
        "        '3': 'hnsw'\n",
        "    }.get(choice, 'flat')\n",
        "\n",
        "    index, chunks_data = main(embeddings_file, chunks_file, index_type)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 568
        },
        "id": "EAhpO01a69Vd",
        "outputId": "b2db079f-c7f9-4f82-e8cf-037475c5b34c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running in Google Colab environment\n",
            "Would you like to:\n",
            "1. Upload the embeddings and chunks files\n",
            "2. Enter the file paths to the embeddings and chunks files\n",
            "Enter your choice (1 or 2): 2\n",
            "Enter path to embeddings file (e.g., /content/chunk_embeddings.npy): /content/chunk_embeddings.npy\n",
            "Enter path to chunks file (e.g., /content/chunks_with_embeddings.json): /content/chunks_with_embeddings.json\n",
            "\n",
            "Choose FAISS index type:\n",
            "1. Flat index (exact search, slower but most accurate)\n",
            "2. IVF index (approximate search, good balance of speed and accuracy)\n",
            "3. HNSW index (very fast, good accuracy, best for large datasets)\n",
            "Enter your choice (1/2/3): 3\n",
            "Loaded embeddings with shape: (1214, 384)\n",
            "Loaded 1214 chunks with embeddings\n",
            "Creating FAISS index for 1214 vectors with 384 dimensions\n",
            "Created HNSW index with 1214 vectors\n",
            "Saved FAISS index to arabic_embeddings.faiss\n",
            "\n",
            "Testing index with query: 'مرحبا'\n",
            "\n",
            "Top 5 results:\n",
            "1. [Score: 0.7461] له.\n",
            "2. [Score: 0.7136] وطن سيد.\n",
            "3. [Score: 0.7083] بها.\n",
            "4. [Score: 0.6992] تالية.\n",
            "5. [Score: 0.6678] المقدمة\n",
            "\n",
            "FAISS indexing complete!\n",
            "Downloading the FAISS index file...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_1b800ecf-77ca-40e8-8657-e7997db0af89\", \"arabic_embeddings.faiss\", 2039394)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "tTBw3yjrDHq5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1cdbd082-d53d-4abe-9fab-b8c0b59321ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r /content/arabic_embeddings.faiss /content/drive/MyDrive/\n",
        "!cp -r /content/chunks_with_embeddings.json /content/drive/MyDrive/\n",
        "!cp -r /content/chunk_embeddings.npy /content/drive/MyDrive/\n",
        "!cp -r /content/chunked_arabic_book.json /content/drive/MyDrive/"
      ],
      "metadata": {
        "id": "TxTOQY3SQ7yP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rank_bm25"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RziN2UQreuBj",
        "outputId": "15ad98b1-a31e-4d9d-d4aa-1888420947cd"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rank_bm25\n",
            "  Downloading rank_bm25-0.2.2-py3-none-any.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rank_bm25) (2.0.2)\n",
            "Downloading rank_bm25-0.2.2-py3-none-any.whl (8.6 kB)\n",
            "Installing collected packages: rank_bm25\n",
            "Successfully installed rank_bm25-0.2.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import faiss\n",
        "import numpy as np\n",
        "import json\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from rank_bm25 import BM25Okapi\n",
        "import re\n",
        "\n",
        "# Load FAISS index\n",
        "index = faiss.read_index('/content/arabic_embeddings.faiss')\n",
        "\n",
        "# Load chunk data\n",
        "with open('/content/chunks_with_embeddings.json', 'r', encoding='utf-8') as f:\n",
        "    chunks_data = json.load(f)\n",
        "chunks = chunks_data['chunks']  # List of dictionaries with 'text' key\n",
        "\n",
        "# Initialize the embedding model\n",
        "model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n",
        "\n",
        "# Prepare BM25 index\n",
        "def tokenize_arabic(text):\n",
        "    \"\"\"Tokenize Arabic text by splitting on whitespace and removing punctuation.\"\"\"\n",
        "    # Remove punctuation and split\n",
        "    tokens = re.findall(r'\\b\\w+\\b', text)\n",
        "    return tokens\n",
        "\n",
        "# Create tokenized corpus for BM25\n",
        "tokenized_corpus = [tokenize_arabic(chunk['text']) for chunk in chunks]\n",
        "bm25 = BM25Okapi(tokenized_corpus)\n",
        "\n",
        "def encode_query(query):\n",
        "    \"\"\"Encode an Arabic query into an embedding.\"\"\"\n",
        "    embedding = model.encode([query], convert_to_tensor=False)\n",
        "    return embedding[0]\n",
        "\n",
        "def search_index(query_embedding, k=5):\n",
        "    \"\"\"Search the FAISS index for top-k similar chunks.\"\"\"\n",
        "    query_embedding = query_embedding / np.linalg.norm(query_embedding)\n",
        "    query_embedding = query_embedding.reshape(1, -1).astype('float32')\n",
        "    distances, indices = index.search(query_embedding, k)\n",
        "    return distances[0], indices[0]\n",
        "\n",
        "def retrieve_bm25(query, k=5):\n",
        "    \"\"\"Retrieve top-k chunks based on BM25 scoring.\"\"\"\n",
        "    tokenized_query = tokenize_arabic(query)\n",
        "    scores = bm25.get_scores(tokenized_query)\n",
        "    # Get top-k indices\n",
        "    top_k_indices = np.argsort(scores)[::-1][:k]\n",
        "    results = []\n",
        "    for idx in top_k_indices:\n",
        "        chunk_text = chunks[idx]['text']\n",
        "        score = scores[idx]\n",
        "        results.append((chunk_text, score))\n",
        "    return results\n",
        "\n",
        "def retrieve_semantic(query, k=5):\n",
        "    \"\"\"Retrieve top-k chunks based on semantic similarity.\"\"\"\n",
        "    query_embedding = encode_query(query)\n",
        "    distances, indices = search_index(query_embedding, k)\n",
        "    results = []\n",
        "    for dist, idx in zip(distances, indices):\n",
        "        chunk_text = chunks[idx]['text']\n",
        "        results.append((chunk_text, dist))\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "5V-9rlXcRCk3"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "exit_program = False\n",
        "while not exit_program:\n",
        "    query = input(\"أدخل استفسارك بالعربية (Enter your Arabic query): \")\n",
        "    k = 5  # Number of top results to show\n",
        "\n",
        "    # Get results\n",
        "    bm25_results = retrieve_bm25(query, k)\n",
        "    semantic_results = retrieve_semantic(query, k)\n",
        "\n",
        "    # Display results in a neat table\n",
        "    print(\"\\n{:<4} {:<55} | {:<55}\".format(\"#\", \"BM25 (نتائج البحث بـ BM25)\", \"Semantic (نتائج البحث الدلالي)\"))\n",
        "    print(\"-\" * 120)\n",
        "\n",
        "    for i in range(k):\n",
        "        # BM25 result\n",
        "        bm25_text = bm25_results[i][0] if i < len(bm25_results) else \"\"\n",
        "        bm25_score = bm25_results[i][1] if i < len(bm25_results) else 0\n",
        "        bm25_trunc = (bm25_text[:52] + \"...\") if len(bm25_text) > 55 else bm25_text\n",
        "\n",
        "        # Semantic result\n",
        "        sem_text = semantic_results[i][0] if i < len(semantic_results) else \"\"\n",
        "        sem_score = semantic_results[i][1] if i < len(semantic_results) else 0\n",
        "        sem_trunc = (sem_text[:52] + \"...\") if len(sem_text) > 55 else sem_text\n",
        "\n",
        "        print(\"{:<4} {:<55} | {:<55}\".format(\n",
        "            f\"{i+1}.\",\n",
        "            f\"[{bm25_score:.4f}] {bm25_trunc}\",\n",
        "            f\"[{sem_score:.4f}] {sem_trunc}\"\n",
        "        ))\n",
        "\n",
        "    print(\"-\" * 120)\n",
        "\n",
        "    # Ask user to continue or exit\n",
        "    while True:\n",
        "        continue_response = input(\"هل تريد إجراء استفسار آخر؟ (ن/ل): \").strip().lower()\n",
        "        if continue_response.startswith('ن') or continue_response.startswith('y'):\n",
        "            break  # Continue\n",
        "        elif continue_response.startswith('ل') or continue_response.startswith('n'):\n",
        "            exit_program = True\n",
        "            break\n",
        "        else:\n",
        "            print(\"الرجاء إدخال 'ن' للمتابعة أو 'ل' للإنهاء.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Txgnv6oRYEL",
        "outputId": "79ae08d7-c20a-4e8d-d40b-1eacd197d279"
      },
      "execution_count": 14,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "أدخل استفسارك بالعربية (Enter your Arabic query): مادة 1\n",
            "\n",
            "#    BM25 (نتائج البحث بـ BM25)                              | Semantic (نتائج البحث الدلالي)                         \n",
            "------------------------------------------------------------------------------------------------------------------------\n",
            "1.   [10.6335] مادة 1                                        | [1.0000] مادة 1                                        \n",
            "2.   [8.8833] ● [1]                                          | [0.8213] مادة 2                                        \n",
            "3.   [6.3916] المحكمة الدستورية العليا في ذلك الشأن. 1       | [0.7500] ● [1]                                         \n",
            "4.   [6.1061] هذا دستورنا. 1 . يتم إيداع االحكام في المضابط  | [0.7196] مادة 5                                        \n",
            "5.   [4.3555] تمارس الحكومة، بوجه خاص، االختصاصات اآلتية: 1  . اال... | [0.7184] مادة 4                                        \n",
            "------------------------------------------------------------------------------------------------------------------------\n",
            "هل تريد إجراء استفسار آخر؟ (ن/ل): ل\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "2ul2NEXGRvY9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "z1t1DWaMoJlF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}