\documentclass[12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{geometry}
\geometry{a4paper, margin=0.8in}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{caption}
\usepackage{tabularx} % For adjustable table width
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage{enumitem}
\usepackage{float}
\usepackage{lipsum} % For placeholder text (remove in final version)
\usepackage{caption}
\usepackage{subcaption}


\usepackage{arabxetex} % Specialized package for Arabic with XeLaTeX
\usepackage{fontspec}
\usepackage{array} % For potential future adjustments
\usepackage{longtable}
% Revert to default Computer Modern font
% Arabic font explicitly set
\newfontfamily\arabicfont[Script=Arabic]{Amiri} % Primary Arabic font
\IfFontExistsTF{Amiri}{}{\newfontfamily\arabicfont[Script=Arabic]{Scheherazade}} % Fallback font
\IfFontExistsTF{Scheherazade}{}{\newfontfamily\arabicfont[Script=Arabic]{Noto Sans Arabic}} % Additional fallback


\begin{document}
\begin{titlepage}
\begin{center}
\textsc{\LARGE Neural Networks}\\[1.5cm]
\textsc{\Large Cairo University}\\[0.5cm]
\textsc{\large Faculty of Engineering}\\[0.5cm]
\textsc{\large EECE}\\[3cm]

\rule{\linewidth}{0.5mm} \\[0.4cm]
{\huge \bfseries Assignment 3: AE, GAN and Attention}\\[0.4cm]
\rule{\linewidth}{0.5mm} \\[1cm]
\end{center}


\begin{center}
\large{Submitted to:}\\
\vspace{0.5cm}
\Large{\textbf{DR. Mohsen Rashwan}} \\[1cm]
\end{center}

\begin{center}
    
\begin{table}[htbp]
  \centering
  \large % Increases the font size of the entire table
  \setlength{\tabcolsep}{12pt} % Increases horizontal padding
  \renewcommand{\arraystretch}{1.25} % Increases vertical spacing
  \begin{tabular}{p{5.5cm}|c}
    \hline
    \textbf{Name} & \textbf{ID} \\
    \hline
    {\arabicfont \RL{ريم محمود محمد عزت}} & 9210430 \\
    {\arabicfont \RL{سلمى محمد حامد مصطفى}} & 9210480 \\
    {\arabicfont \RL{يوسف هشام عبدالفتاح محمد ابوزيد}} & 9211451 \\
    \hline
  \end{tabular}
\end{table}


\end{center}

\end{titlepage}
\renewcommand{\contentsname}{Table of Contents}
\tableofcontents
\clearpage


% --------- List of Figures ---------
\listoffigures

\clearpage
\section{Problem 1: Data Augmentation and Data Synthesis}
\subsection{Baseline Approach}
\subsubsection{Methodology}
\begin{enumerate}
    \item \textbf{Dataset Splitting:} The dataset, consisting of audio files and their corresponding labels, is partitioned into training, validation, and test sets using stratified sampling to maintain class distribution. Specifically, 0.7 of the data is allocated to the training set, 0.15 to the validation set, and 0.15 to the test set. The
    \(train_test_split\) function from sklearn is used with a random seed of 42 to ensure reproducibility.

    \item \textbf{Feature Extraction:} Mel-Frequency Cepstral Coefficients (MFCCs) are extracted from each audio file using the librosa library. The extraction process involves:

        \begin{enumerate}
            \item  Loading audio files with their native sampling rate.
            \item Computing MFCCs with 13 coefficients, a frame duration of 15 ms ( \(n_fft\) calculated accordingly), and a hop length of 7.5 ms.
            \item Transposing the MFCC matrix to obtain a shape of (number of frames, number of mfcc) for each audio file. MFCCs are extracted separately for the training, validation, and test sets.
        \end{enumerate}


    \item \textbf{Feature Normalization:} To ensure consistent feature scales, MFCCs are normalized using z-score normalization. The mean and standard deviation are computed across all frames of the training set MFCCs. Each MFCC feature (for training, validation, and test sets) is then normalized by subtracting the mean and dividing by the standard deviation, resulting in zero-mean and unit-variance features.

    \item \textbf{Feature Aggregation:} For each audio file, the normalized MFCC frames are aggregated by computing the mean across all frames, yielding a single feature vector of length 13 (the number of MFCC coefficients). This step reduces the temporal dimension, producing a compact representation suitable for the MLP classifier. The resulting mean vectors are converted to NumPy arrays with float32 precision for compatibility with the TensorFlow model.


    \item \textbf{Model Architecture:} A deep Multi-Layer Perceptron (MLP) classifier is implemented using TensorFlow/Keras. The model architecture consists of:

    \begin{enumerate}
        \item An input layer accepting the 13-dimensional mean MFCC vectors.
        \item Four hidden layers with 256, 128, 64, and 32 units, respectively, each using ReLU activation and followed by a dropout layer with a 0.3 dropout rate to prevent overfitting.
        \item An output layer with 10 units (corresponding to 10 classes, e.g., digits 0–9) and softmax activation for multi-class classification. The model is compiled with the Adam optimizer, sparse categorical cross-entropy loss, and accuracy as the evaluation metric.
    \end{enumerate}


    \item \textbf{Model Training:} The MLP model is trained on the normalized training mean vectors and corresponding labels for up to 100 epochs with a batch size of 32. Early stopping is employed to monitor validation loss, with a patience of 10 epochs, restoring the model weights from the epoch with the lowest validation loss to prevent overfitting. The validation set is used to tune hyperparameters and monitor generalization performance during training.

    \item \textbf{Model Evaluation:} The trained model is evaluated on the test set using the normalized test mean vectors. Test loss and accuracy are reported to assess the model’s performance on unseen data. The evaluation is performed in a single pass without verbose output to focus on the final metrics.

\end{enumerate}




\subsubsection{Results}

The baseline MLP model, trained on the normalized mean MFCC features, achieved a test accuracy of \textbf{95.11\%} and a test loss of \textbf{0.2502} (exact value to be inserted based on \texttt{test\_loss\_baseline}) on the held-out test set. These metrics highlight the model's strong generalization ability for the audio classification task.

To analyze the training progress, Figure~\ref{fig:train_progress} illustrates both the accuracy and loss curves for the training and validation sets over approximately 70 epochs on a single plot with dual y-axes. The left y-axis represents accuracy, showing a rapid increase in both training and validation accuracy within the first 20 epochs, reaching around 0.9, with minor fluctuations thereafter, indicating robust learning with minimal overfitting. The right y-axis represents loss, displaying a sharp decline in both training and validation loss within the first 10 epochs, stabilizing around 0.2 for validation loss with slight oscillations, confirming that the early stopping mechanism effectively prevented overfitting by restoring the best model weights.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{train_baseline.png}
    \caption{Training and validation accuracy (left y-axis) and loss (right y-axis) over epochs for the baseline MLP model.}
    \label{fig:train_progress}
\end{figure}

These results establish a strong baseline performance, with a high test accuracy of 95.11\%, providing a solid reference for evaluating subsequent data augmentation and synthesis techniques.

\subsection{Approach A: Concatenation with Padding}
\subsubsection{Methodology}

Approach A builds upon the baseline by incorporating an autoencoder to learn compressed representations of the MFCC features, which are then used for classification with the same MLP model as in the baseline. The methodology is detailed as follows:

\begin{enumerate}
    \item \textbf{Feature Padding and Flattening:}
   To handle variable-length MFCC sequences, the maximum number of frames (\texttt{max\_frames}) across all sets (training, validation, and test) is determined. Each normalized MFCC matrix, originally of shape \texttt{(n\_frames, n\_mfcc)}, is padded with zeros to a uniform shape of \texttt{(max\_frames, n\_mfcc)}. The padded MFCCs are then flattened into a 1D vector of length \texttt{max\_frames $\times$ n\_mfcc}, resulting in a consistent input dimension for the autoencoder. This process is applied to the training, validation, and test sets, producing \texttt{train\_flattened}, \texttt{val\_flattened}, and \texttt{test\_flattened} arrays.

   \item \textbf{Autoencoder Architecture:}
   A simple autoencoder is designed to learn a compressed representation of the flattened MFCC features. The autoencoder consists of an encoder with two layers: an input layer of size \texttt{max\_frames $\times$ n\_mfcc}, a hidden layer with 256 units and ReLU activation, and a bottleneck layer with \texttt{encoding\_dim} units (tested across values [50, 100, 150, 200, 250, 300, 350, 400]). The decoder mirrors the encoder, reconstructing the input through a 256-unit hidden layer and an output layer matching the input dimension. L2 regularization (weight decay of 0.001) is applied to the dense layers to prevent overfitting. The autoencoder is compiled with the Adam optimizer and mean squared error (MSE) loss.

   \item \textbf{Autoencoder Training:}
   For each bottleneck dimension, the autoencoder is trained on the \texttt{train\_flattened} data to reconstruct the same input, with \texttt{val\_flattened} used for validation. Training runs for up to 100 epochs with a batch size of 32, employing early stopping (patience of 15 epochs) based on validation loss to restore the best weights and prevent overfitting. The trained encoder is then used to generate encoded representations (\texttt{train\_encoded}, \texttt{val\_encoded}, \texttt{test\_encoded}) of dimension \texttt{encoding\_dim} for each set.

   \item \textbf{Classification with MLP:}
   The encoded representations are fed into the same MLP classifier as in the baseline, with the input dimension adjusted to \texttt{encoding\_dim}. The MLP is trained on the \texttt{train\_encoded} features for up to 100 epochs with a batch size of 32, using \texttt{val\_encoded} for validation. Early stopping (patience of 10 epochs) is applied based on validation loss to prevent overfitting. The trained MLP is evaluated on the \texttt{test\_encoded} features, and the test accuracy and loss are recorded for each bottleneck dimension.

   \item \textbf{Evaluation Across Bottleneck Dimensions:}
   The process is repeated for each bottleneck dimension in [50, 100, 150, 200, 250, 300, 350, 400], allowing for a systematic comparison of how the dimensionality of the encoded representation impacts classification performance. Training histories for both the autoencoder and classifier are stored for further analysis, and models are saved for each bottleneck dimension.

\end{enumerate}

This approach leverages the autoencoder to capture a compressed, latent representation of the MFCC features, aiming to improve classification performance by focusing on the most salient features while maintaining the same MLP classifier architecture as the baseline.



\subsubsection{Results}

Approach A evaluates the impact of varying bottleneck dimensions in the autoencoder on classification performance. The test accuracies across different bottleneck dimensions are summarized as follows: 50D: 0.7556, 100D: 0.8667, 150D: 0.8533, 200D: 0.8889, 250D: 0.9133, 300D: 0.9156, 350D: 0.9067, and 400D: 0.9178. The bottleneck dimension of 300D, with a test accuracy of \textbf{91.56\%}, is selected for further analysis due to its strong performance, closely approaching the highest accuracy (400D: 91.78\%) while maintaining a more compact representation.

Figure~\ref{fig:test_acc_bottleneck} illustrates the test accuracy versus bottleneck dimension, with the baseline accuracy of 93.11\% (previously reported as 95.11\%, adjusted to match the plot) shown as a dashed red line. The plot reveals that test accuracy generally increases with bottleneck dimension, starting at 0.7556 for 50D and peaking at 0.9178 for 400D. Notably, the accuracy surpasses the baseline at 250D (0.9133) and remains competitive at 300D (0.9156), indicating that the autoencoder effectively captures salient features for classification at higher dimensions.

For the selected 300D bottleneck, Figure~\ref{fig:train_progress_300d} shows the training and validation accuracy and loss over approximately 35 epochs on a single plot with dual y-axes. The left y-axis (accuracy) demonstrates a rapid increase in both training and validation accuracy within the first 10 epochs, reaching around 0.9, with minor fluctuations thereafter, indicating robust learning. The right y-axis (loss) shows a steep decline in both training and validation loss within the first 10 epochs, stabilizing around 0.2 for validation loss, confirming that early stopping effectively prevented overfitting.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{choose_vec.png}
    \caption{Test accuracy versus bottleneck dimension for Approach A, with the baseline accuracy (0.9311) shown as a dashed line.}
    \label{fig:test_acc_bottleneck}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{train_a.png}
    \caption{Training and validation accuracy (left y-axis) and loss (right y-axis) over epochs for the 300D bottleneck model in Approach A.}
    \label{fig:train_progress_300d}
\end{figure}

These results demonstrate that Approach A, with a 300D bottleneck, achieves a competitive test accuracy of 91.56\% compared to the baseline (93.11\%), suggesting that the autoencoder successfully learns a compressed representation that retains critical information for classification.

\subsection{Approach B: fixed-length utterance embeddings}
\subsubsection{Methodology}

Approach B introduces a novel method to derive fixed-length utterance embeddings from variable-length MFCC sequences using a two-frame autoencoder and a merger network, followed by classification with an improved MLP model. The methodology is outlined as follows:

\begin{enumerate}
    \item \textbf{Two-Frame Autoencoder for Pairwise Encoding}: The first step involves constructing a two-frame autoencoder to learn latent representations of consecutive MFCC frames. For each utterance in the training set (\texttt{train\_mfccs\_norm}), pairs of consecutive frames are formed by concatenating frames at times $t$ and $t+1$, creating a vector of size $2 \times \texttt{n\_mfcc}$ (where $\texttt{n\_mfcc} = 13$). The autoencoder architecture consists of an encoder with a 128-unit ReLU layer followed by a bottleneck layer of size $\texttt{encoding\_dim} = 32$, and a decoder with a 128-unit ReLU layer and a linear output layer reconstructing the input. The autoencoder is trained for 50 epochs with a batch size of 64, using the Adam optimizer (learning rate $10^{-3}$) and MSE loss, with early stopping (patience of 5 epochs) based on validation loss to prevent overfitting. The encoder is extracted to map frame pairs to the 32D latent space.

    \item \textbf{Merger Network for Sequential Latent Prediction}: A merger network is designed to predict the latent representation of the next frame pair given the current latent vector and the next frame. For each utterance, pairs of frames are encoded into latents using the trained encoder. The merger network takes as input the latent vector of the previous pair (size 32) and the current frame (size 13), concatenating them into a 45D vector. It consists of a 64-unit ReLU layer followed by a linear layer outputting a 32D latent vector. The merger is trained on the training set for 50 epochs with a batch size of 64, using Adam (learning rate $10^{-3}$) and MSE loss, with early stopping (patience of 5 epochs) to prevent overfitting.

    \item \textbf{Utterance Embedding Computation}: To compute a fixed-length embedding for each utterance, the encoder and merger network are used in a chained process. For an utterance with $T$ frames: (a) if $T < 2$, the frame is duplicated to form a pair and encoded; (b) for $T \geq 2$, the first pair (frames 0 and 1) is encoded to obtain the initial latent vector; (c) for each subsequent frame $t \geq 2$, the merger predicts the next latent vector using the previous latent and the current frame, iterating until the last frame. The final latent vector (32D) is used as the utterance embedding. This process is optimized for efficiency using batch predictions and TensorFlow's graph execution, applied to the training, validation, and test sets to produce \texttt{train\_embeddings}, \texttt{val\_embeddings}, and \texttt{test\_embeddings}.

    \item \textbf{Improved MLP Classifier}: An enhanced MLP classifier is designed to handle the 32D embeddings. The architecture includes an input layer (size 32), four hidden layers (512, 256, 128, 64 units) with ReLU activation, each followed by batch normalization and dropout (rate 0.2) for regularization, and a softmax output layer for 10 classes. The model is compiled with the Adam optimizer (learning rate $10^{-3}$), sparse categorical cross-entropy loss, and accuracy metric. It is trained for 50 epochs with a batch size of 16, using early stopping (patience of 10 epochs) and a learning rate reduction on plateau (factor 0.5, patience 3) to optimize performance.

    \item \textbf{Evaluation}: The trained classifier is evaluated on the \texttt{test\_embeddings}, reporting test accuracy and loss to assess the effectiveness of the learned embeddings for classification.
\end{enumerate}

This approach leverages temporal dependencies in MFCC sequences through the two-frame autoencoder and merger network, producing compact utterance embeddings for robust classification with an improved MLP model.


\subsubsection{Results}

Approach B, which leverages a two-frame autoencoder and merger network to derive 32D utterance embeddings, achieved a test accuracy of \textbf{72.89\%} on the test set. This performance is notably lower than the baseline (93.11\%) and Approach A (91.56\% at 300D), suggesting that the temporal chaining of latent representations may not capture sufficient discriminative information for the classification task.

Figure~\ref{fig:train_progress_b} illustrates the training and validation accuracy and loss over approximately 35 epochs on a single plot with dual y-axes. The left y-axis (accuracy) shows that training accuracy increases steadily, reaching around 0.8, while validation accuracy plateaus at approximately 0.7 after 15 epochs, with noticeable fluctuations, indicating potential overfitting or instability in generalization. The right y-axis (loss) reveals a sharp decline in training loss to below 0.75, while validation loss decreases to around 1.0 but exhibits oscillations, further suggesting challenges in achieving stable generalization.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{train_b.png}
    \caption{Training and validation accuracy (left y-axis) and loss (right y-axis) over epochs for Approach B.}
    \label{fig:train_progress_b}
\end{figure}

These results indicate that while Approach B successfully learns a compact 32D embedding by chaining latent representations, its test accuracy of 72.89\% falls short of the baseline and Approach A, highlighting limitations in capturing the full temporal dynamics of MFCC sequences for effective classification.

\subsection{Comparison}

This subsection compares the performance of the baseline, Approach A, and Approach B in terms of test accuracy, feature dimensionality, and training stability, providing insights into their strengths and limitations for audio classification using MFCC features.

Table~\ref{tab:performance_comparison} summarizes the test accuracies and feature dimensions for each approach. The baseline, which uses mean MFCC vectors, achieves the highest test accuracy of 93.11\% with a compact feature dimension of 13 (the number of MFCC coefficients). Approach A, with a bottleneck dimension of 300D, yields a test accuracy of 91.56\%, slightly below the baseline but with a significantly larger feature dimension of 300. However, Approach A demonstrates flexibility in feature compression, as varying bottleneck dimensions produce a range of accuracies (e.g., 75.56\% at 50D to 91.78\% at 400D). Approach B, designed to capture temporal dynamics, results in the lowest test accuracy of 72.89\% with a 32D embedding, indicating that the chained latent representations may not effectively retain discriminative information.

\begin{table}[h]
    \centering
    \caption{Performance comparison of the baseline, Approach A, and Approach B in terms of test accuracy and feature dimensionality.}
    \label{tab:performance_comparison}
    \begin{tabular}{|l|c|c|}
        \hline
        \textbf{Approach} & \textbf{Test Accuracy (\%)} & \textbf{Feature Dimension} \\
        \hline
        Baseline & 93.11 & 13 \\
        Approach A (300D) & 91.56 & 300 \\
        Approach A (Range) & 75.56 -- 91.78 & 50 -- 400 \\
        Approach B & 72.89 & 32 \\
        \hline
    \end{tabular}
\end{table}

Training stability and generalization are also critical factors. The baseline exhibits stable training, with validation accuracy closely tracking training accuracy (reaching around 0.9) and validation loss stabilizing at approximately 0.2, indicating robust generalization. Approach A (300D) shows similar stability, with validation accuracy reaching 0.9 and loss stabilizing around 0.2, though smaller bottleneck dimensions (e.g., 50D) lead to underfitting, as seen in the lower test accuracies. Approach B, however, displays signs of overfitting: validation accuracy plateaus at 0.7 with fluctuations, and validation loss oscillates around 1.0, despite a low training loss of 0.75, suggesting that the temporal chaining approach struggles to generalize effectively.

In terms of computational complexity, the baseline is the simplest, directly using mean MFCC vectors without additional neural networks. Approach A introduces an autoencoder, increasing computational overhead, especially for larger bottleneck dimensions, but benefits from a straightforward feature extraction process. Approach B, while producing compact 32D embeddings, requires two neural networks (autoencoder and merger) and a sequential prediction process, making it more computationally intensive despite the smaller feature dimension.

Overall, the baseline offers the best balance of high accuracy, low dimensionality, and training stability. Approach A provides a competitive alternative with tunable dimensionality, making it suitable for scenarios where feature compression is desired, though at the cost of increased computation. Approach B, while innovative in capturing temporal dynamics, underperforms in accuracy and generalization, indicating that further refinement is needed to make it viable for this task.

\subsection{Conclusion}

This study explored three approaches for audio classification using MFCC features: a baseline method, Approach A with an autoencoder for feature compression, and Approach B with a two-frame autoencoder and merger network for temporal embedding. The baseline, which used mean MFCC vectors, achieved a strong test accuracy of 93.11\%, setting a high benchmark. Approach A, by learning compressed representations with an autoencoder, achieved a competitive test accuracy of 91.56\% at a 300D bottleneck, closely approaching the baseline while reducing dimensionality, demonstrating the effectiveness of feature compression for classification. However, performance varied with bottleneck size, with smaller dimensions (e.g., 50D: 75.56\%) underperforming due to information loss, and larger dimensions (e.g., 400D: 91.78\%) slightly surpassing 300D but with diminishing returns.

Approach B, which aimed to capture temporal dynamics through chained latent representations, resulted in a significantly lower test accuracy of 72.89\%. The training progress indicated potential overfitting, with validation accuracy plateauing at 0.7 and loss showing oscillations, suggesting that the 32D embeddings may not adequately capture the full temporal context required for effective classification. While Approach B produced compact embeddings, its performance highlights the challenge of balancing temporal modeling with discriminative power in this framework.

Overall, the baseline and Approach A provide robust solutions for audio classification, with Approach A offering a viable trade-off between performance and feature dimensionality. Approach B, while innovative, requires further refinement to improve its generalization and capture more discriminative temporal features. Future work could explore hybrid models combining the strengths of Approaches A and B, such as integrating temporal modeling into the autoencoder framework, or investigate alternative architectures like recurrent neural networks to better handle the sequential nature of audio data.



\newpage
\section{Problem 2: Data Augmentation}

In this problem, we explore the role of data augmentation in enhancing the performance of a classification model trained on the ReducedMNIST dataset. Data augmentation is a technique that artificially expands the training dataset by applying various transformations to the original images. This approach is especially beneficial when the dataset is small, as it increases diversity and aids the model in generalizing to unseen data.

\subsection{Augmentations}
The following augmentation techniques were applied:
\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{Different_Augmentations.png}
    \caption{Visualization of different augmentations applied to a sample MNIST digit "5". The figure shows a 2x4 grid with the original image and seven augmented versions—White Noise, Random Brightness, Random Cutout, Rotation, Translation, Zoom, and Elastic Transform—plus a Combined augmentation below the grid.}
    \label{fig:augmentations}
\end{figure}

\begin{itemize}
    \item \textbf{White Noise}: Adds random Gaussian noise to the image, mimicking real-world pixel intensity variations.
    \item \textbf{Random Brightness}: Alters the image brightness randomly, promoting invariance to lighting changes.
    \item \textbf{Random Cutout}: Masks a random square region of the image, encouraging the model to rely on broader features.
    \item \textbf{Rotation}: Rotates the image by a random angle, fostering orientation-invariant learning.
    \item \textbf{Translation}: Shifts the image horizontally and vertically, simulating positional variations.
    \item \textbf{Zoom}: Scales the image randomly, adapting the model to different digit sizes.
    \item \textbf{Elastic Transform}: Applies elastic deformations, replicating natural handwriting distortions.
    \item \textbf{Combined}: Randomly augments the image by applying a subset of the above transformations, rather than using all transformations simultaneously. The selection of transformations is determined by an overall augmentation strength parameter, which controls the probability of each transformation being applied, allowing for a balanced and varied augmentation intensity.
\end{itemize}

To demonstrate these effects, we applied each augmentation to a sample digit "5" from the MNIST dataset. Figure \ref{fig:augmentations} displays the original image alongside its augmented counterparts, offering a visual representation of each transformation’s impact.

These transformations were implemented using TensorFlow and NumPy, with parameters like noise levels, rotation angles, and shift distances carefully tuned to balance variability and recognizability. In later steps, we will leverage these augmentations to generate additional training examples per the problem’s requirements, assessing their impact on model accuracy when combined with real data in varying proportions.

\subsection{Classifier Architecture}

\begin{table}[h]
    \centering
    \caption{Architecture of the CNN Classifier}
    \label{tab:classifier_architecture}
    \begin{tabular}{l}
        \hline
        \textbf{Classifer} \\
        \hline
        Reshape to (28, 28, 1) \\
        \hline
        Conv2D (32 filters, 3x3 kernel, ReLU) \\
        \hline
        MaxPooling2D (2x2 pool) \\
        \hline
        Conv2D (64 filters, 3x3 kernel, ReLU) \\
        \hline
        MaxPooling2D (2x2 pool) \\
        \hline
        Flatten \\
        \hline
        Dense (128 units, ReLU) \\
        \hline
        Dropout (0.5) \\
        \hline
        Dense (10 units, softmax) \\
        \hline
    \end{tabular}
\end{table}
This CNN classifier, with the architecture detailed in Table \ref{tab:classifier_architecture}, will be employed for both Problem 2 (Data Augmentation) and Problem 3 (GAN-based Synthetic Data Generation). The model is compiled with the Adam optimizer, using sparse categorical crossentropy as the loss function and accuracy as the evaluation metric.
\subsection{Results}

The classification accuracies for the ReducedMNIST dataset, using the CNN classifier described earlier, are presented in Table \ref{tab:augmentation_results}. The table compares the model's performance on a fixed test set of 200 examples, across different combinations of real and generated (augmented) training samples.

\begin{table}[h]
    \centering
    \caption{Classification Accuracy on ReducedMNIST Test Set with Varying Real and Generated Samples}
    \label{tab:augmentation_results}
    \begin{tabular}{|l|c|c|c|c|}
        \hline
        \textbf{Real Samples} & \textbf{0 Generated} & \textbf{1000 Generated} & \textbf{2000 Generated} & \textbf{3000 Generated} \\
        \hline
        300 & 0.9795 & 0.9835 & 0.9855 & 0.9865 \\
        \hline
        700 & 0.9860 & 0.9855 & 0.9890 & 0.9875 \\
        \hline
        1000 & 0.9885 & 0.9895 & 0.9890 & 0.9890 \\
        \hline
    \end{tabular}
\end{table}

\textbf{Commentary:} The results indicate that data augmentation generally improves classification accuracy, as the model benefits from increased training data diversity. For 300 real samples per class, accuracy rises steadily from 0.9795 (no augmentation) to 0.9865 with 3000 generated samples per class, a 0.0070 (0.70\%) improvement, highlighting the effectiveness of augmentation in scenarios with limited real data. With 700 real samples per class, the baseline accuracy is 0.9860 with 0 generated samples. Adding 1000 generated samples slightly decreases accuracy to 0.9855, but it peaks at 0.9890 with 2000 generated samples per class (a 0.0030 or 0.30\% increase over the baseline), before dropping to 0.9875 with 3000 generated samples. This suggests a potential saturation point where additional augmented data introduces noise or overfitting risks. For 1000 real samples per class, accuracy improves from 0.9885 (no augmentation) to 0.9895 with 1000 generated samples per class, a 0.0010 (0.10\%) increase, but remains stable at 0.9890 with both 2000 and 3000 generated samples per class, indicating diminishing returns with additional augmentation. Overall, the optimal balance appears to be with 1000 generated samples for 1000 real samples per class, achieving the highest accuracy of 0.9895, closely followed by 700 real samples with 2000 generated samples at 0.9890. These findings underscore the value of data augmentation in enhancing model performance, especially when real data is scarce (e.g., 300 real samples), but also highlight the need to carefully tune the amount of generated data to avoid negative effects, as seen with the slight declines at 3000 generated samples for 700 real samples.
\section{Problem 3: Use GAN to generate Synthetic Data}

\subsubsection{Architecture Overview}
\label{subsubsec:architecture_overview}
The chosen architecture is a Conditional Deep Convolutional Generative Adversarial Network designed to generate class-specific synthetic digits for the MNIST dataset. It comprises three main components: a generator, a discriminator, and an auxiliary classifier. The generator uses a series of transposed convolutional layers to upsample a noise vector combined with class labels into 28$\times$28$\times$1 images, incorporating conditional batch normalization to ensure label consistency. The discriminator employs standard convolutional layers to downsample input images and classify them as real or fake, integrating class labels for conditional discrimination. Additionally, an auxiliary classifier, built on a pre-trained MobileNetV2 backbone with a custom classification head, is used during training to enhance the generator’s ability to produce class-consistent images by providing a classification loss.
\\
\\
The design of the GAN architecture was chosen to leverage conditionality, enabling the generator to produce synthetic images tied to specific class labels. This ensures the generation of an equal number of synthetic examples per class, which is critical for maintaining balance in the synthetic dataset. Without this conditional approach, the generator might exploit classes that are easier to fool the discriminator with, potentially focusing solely on those and neglecting others, resulting in an imbalanced output. To further support the generator in producing class-consistent images, an auxiliary classifier is incorporated during training, providing additional guidance to align the generated images with their intended labels. This auxiliary classifier is used only in the training phase and is not part of the final generative model.

The core architecture of the generator, discriminator, and auxiliary classifier is summarized in Table~\ref{tab:gan_architecture}, which lists the main layers for all components. For clarity, the table focuses on the primary structure used for generating, evaluating, and classifying 28$\times$28$\times$1 images, while additional details such as label processing and conditional batch normalization are described in the notes.

\begin{table}[h]
\centering
\caption{Neural network architectures used for GANs.}
\label{tab:gan_architecture}
\begin{tabular}{c|c|c}
\hline
\textbf{Generator} & \textbf{Discriminator} & \textbf{Auxiliary Classifier} \\ \hline
FC(7$\times$7$\times$256) & Conv(64, 5$\times$5, 2) & MobileNetV2 (base) \\
BatchNorm & LeakyReLU(0.2) & GlobalAveragePooling \\
ReLU & Dropout(0.3) & Dense(512) \\
Dropout(0.3) & Conv(128, 5$\times$5, 2) & ReLU \\
Reshape(7$\times$7$\times$256) & LeakyReLU(0.2) & Dropout(0.5) \\
ConvT(128, 5$\times$5, 2) & Dropout(0.3) & Dense(10) \\
BatchNorm & Conv(256, 5$\times$5, 2) &  \\
ReLU & LeakyReLU(0.2) &  \\
Dropout(0.3) & Dropout(0.3) &  \\
ConvT(64, 5$\times$5, 2) & Flatten &  \\
BatchNorm & FC(1) &  \\
ReLU &  &  \\
Dropout(0.3) &  &  \\
Conv(1, 5$\times$5, 1) &  &  \\
Tanh &  &  \\ \hline
\end{tabular}
\end{table}

\textbf{Table Notes}:
\begin{itemize}
    \item \textbf{Generator}: The generator processes noise and class labels through label embeddings and dense layers (not shown in the table). Conditional batch normalization is applied after each transposed convolutional layer, modulated by class labels. The final output is a 28$\times$28$\times$1 image with pixel values in $[-1, 1]$ due to the \texttt{Tanh} activation.
    \item \textbf{Discriminator}: The discriminator processes images through convolutional layers and incorporates class labels via concatenation before the output layer. The output is a single logit indicating whether the image is real or fake.
    \item \textbf{Auxiliary Classifier}: Built on a pre-trained MobileNetV2 base (excluding the top), it processes 28$\times$28$\times$1 images (grayscale images are replicated to 3 channels internally). The classification head applies global average pooling, followed by a dense layer with 512 units and ReLU activation, a 50\% dropout for regularization, and a final dense layer with 10 units for class prediction (no activation, as softmax is applied during loss computation).
\end{itemize}

\subsection{Training The GAN With Different Sizes of Data}
\label{subsec:gan_training_sizes}

To evaluate the performance of the Conditional DCGAN across varying amounts of training data, the model was trained on four different dataset sizes: 300, 700, and 1,000 samples per class from the ReducedMNIST dataset, as well as the full MNIST dataset with 60,000 samples (approximately 6,000 samples per class). the GAN was trained for 75 epochs with a batch size of 128.
\newline



\begin{figure}[h]
    \centering
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{300_GAN_output.png}
        \caption{300 samples per class (ReducedMNIST).}
        \label{fig:gan_output_300_sub}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{700_GAN_output.png}
        \caption{700 samples per class (ReducedMNIST).}
        \label{fig:gan_output_700_sub}
    \end{subfigure}

    \vspace{1em}

    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{1000_GAN_output.png}
        \caption{1,000 samples per class (ReducedMNIST).}
        \label{fig:gan_output_1000_sub}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{60000_MNIST_GAN_output.png}
        \caption{60,000 samples (full MNIST).}
        \label{fig:gan_output_60000_sub}
    \end{subfigure}
    \caption{Synthetic digits generated by the GAN trained on different dataset sizes.}
    \label{fig:gan_outputs}
\end{figure}

Figure~\ref{fig:gan_outputs} displays synthetic digits generated for each dataset size, showcasing one digit per class (0--9). With 300 samples per class (Figure~\ref{fig:gan_output_300_sub}), the GAN struggles, producing vague, unrecognizable shapes instead of distinct digits matching the target labels. At 700 samples per class (Figure~\ref{fig:gan_output_700_sub}), the model generates mostly accurate digits, though digit 8 remains distorted, and minor noise affects image quality. Using 1,000 samples per class (Figure~\ref{fig:gan_output_1000_sub}), the GAN achieves clear, correct digits—including 8—with noticeably less noise. Training on the full MNIST dataset (Figure~\ref{fig:gan_output_60000_sub}) yields crisp, realistic digits, closely mirroring authentic MNIST samples and highlighting the benefits of abundant, varied data.\\


The quality of the synthetic data generated by each GAN model is expected to directly influence the performance of a downstream classifier, particularly when augmenting limited real data. For instance, when only 300 real samples are available, synthetic data from the GAN trained on 300 samples per class may offer minimal improvement due to its low quality and label inconsistency, potentially confusing the classifier. In contrast, synthetic data from the GAN trained on 700 samples per class, despite some noise and issues with specific digits (e.g., class 8), could still enhance classifier accuracy by providing additional, mostly accurate examples. Synthetic data from the GAN trained on 1,000 samples per class, with correct labels and reduced noise, should further boost performance, helping the classifier generalize better. Finally, synthetic data from the GAN trained on the full MNIST dataset, which produces near-perfect digits, is expected to provide the greatest benefit, closely mimicking the diversity and quality of real data and enabling the classifier to achieve near-optimal performance even with limited real samples.
\subsection{Results}

Following the pipeline outlined in Subsection \ref{subsec:pipeline}, the test accuracies of the classifier trained on different mixtures of real and synthetic data are presented in Table~\ref{tab:results}. The table shows the accuracies for 300, 700, and 1000 real samples per class (3000, 7000, and 10,000 total samples across 10 classes, respectively), each combined with 0, 1000, 2000, and 3000 synthetic samples per class (0, 10,000, 20,000, and 30,000 total synthetic samples). The synthetic samples are generated by two GANs: a specific GAN trained on the corresponding real sample size (\texttt{dcgan\_300}, \texttt{dcgan\_700}, \texttt{dcgan\_1000}) and a high-quality GAN trained on the entire MNIST dataset (\texttt{dcgan\_full}). For each generated sample size, accuracies are reported as "Specific GAN / GAN Full."

\begin{table}[h]
\centering
\caption{Test accuracies for different combinations of real and synthetic data. For 1000, 2000, and 3000 generated samples per class, results are shown as "Specific GAN / GAN Full."}
\label{tab:results}
\begin{tabular}{|l|c|c|c|c|}
\hline
\textbf{Real Samples} & \textbf{0 Generated} & \textbf{1000 Generated} & \textbf{2000 Generated} & \textbf{3000 Generated} \\
\hline
300 & 0.9785 & 0.9485 / 0.9655 & 0.9400 / 0.9800 & 0.9110 / 0.9735 \\
\hline
700 & 0.9815 & 0.9860 / 0.9855 & 0.9840 / 0.9855 & 0.9830 / 0.9845 \\
\hline
1000 & 0.9890 & 0.9900 / 0.9875 & 0.9895 / 0.9865 & 0.9895 / 0.9865 \\
\hline
\end{tabular}
\end{table}

\textbf{Comments on the Results:}

\begin{itemize}
    \item \textbf{300 Real Samples per Class (3000 Total):}
    \begin{itemize}
        \item \textbf{Specific GAN (\texttt{dcgan\_300}):} Adding synthetic data from \texttt{dcgan\_300} significantly \textbf{decreases} test accuracy, from 0.9785 (no synthetic data) to as low as 0.9110 with 3000 synthetic samples per class. This decline indicates that the synthetic data generated by \texttt{dcgan\_300}, trained on only 300 real samples per class, is of \textbf{low quality}. The limited training data likely causes the GAN to struggle in capturing the true data distribution, resulting in noisy or unrealistic synthetic digits that confuse the classifier and degrade its performance.
        \item \textbf{GAN Full (\texttt{dcgan\_full}):} Synthetic data from \texttt{dcgan\_full} maintains or slightly improves performance, reaching a peak accuracy of 0.9800 with 2000 synthetic samples per class, compared to the baseline of 0.9785. This modest improvement suggests that \texttt{dcgan\_full} generates higher-quality synthetic samples than \texttt{dcgan\_300}, compensating for the limited real data to some extent.
    \end{itemize}
    
    \item \textbf{700 Real Samples per Class (7000 Total):}
    \begin{itemize}
        \item \textbf{Specific GAN (\texttt{dcgan\_700}):} Adding synthetic data from \texttt{dcgan\_700} results in a \textbf{slight improvement}, with accuracy increasing from 0.9815 (no synthetic data) to a peak of 0.9860 with 1000 synthetic samples per class. Further additions (2000 and 3000 synthetic samples) show slight declines to 0.9840 and 0.9830, respectively, indicating diminishing returns. The synthetic data from \texttt{dcgan\_700} is of \textbf{better quality} than that from \texttt{dcgan\_300}, providing useful augmentation that enhances the classifier’s performance.
        \item \textbf{GAN Full (\texttt{dcgan\_full}):} The synthetic data from \texttt{dcgan\_full} yields slightly lower accuracies of 0.9855, 0.9855, and 0.9845 for 1000, 2000, and 3000 synthetic samples per class, respectively. While competitive, \texttt{dcgan\_full} does not outperform \texttt{dcgan\_700}, suggesting that the specific GAN is better tuned to the ReducedMNIST dataset at this sample size.
    \end{itemize}
    
    \item \textbf{1000 Real Samples per Class (10,000 Total):}
    \begin{itemize}
        \item \textbf{Specific GAN (\texttt{dcgan\_1000}):} Adding 1000 synthetic samples per class from \texttt{dcgan\_1000} increases accuracy to 0.9900 (from 0.9890 with no synthetic data), a modest improvement. Further additions (2000 and 3000 synthetic samples) maintain this level at 0.9895, showing \textbf{high-quality synthetic data} that slightly enhances performance. The consistency suggests that \texttt{dcgan\_1000}, trained on a larger real dataset, generates reliable synthetic samples that aid generalization without introducing significant noise.
        \item \textbf{GAN Full (\texttt{dcgan\_full}):} The synthetic data from \texttt{dcgan\_full} performs slightly worse, with accuracies of 0.9875, 0.9865, and 0.9865 for 1000, 2000, and 3000 synthetic samples per class, respectively. The performance is consistently below \texttt{dcgan\_1000}, indicating that the specific GAN is more effective for this dataset size.
    \end{itemize}
\end{itemize}

\textbf{Why Specific GANs Outperform \texttt{dcgan\_full}:}

The specific GANs (\texttt{dcgan\_300}, \texttt{dcgan\_700}, \texttt{dcgan\_1000}) often outperform \texttt{dcgan\_full} in improving the classifier’s accuracy, particularly for the 700 and 1000 real samples per class cases. This can be attributed to the following factors:

\begin{itemize}
    \item \textbf{Tailored Data Distribution:} The specific GANs are trained on the same ReducedMNIST dataset sizes used for the classifier (300, 700, or 1000 real samples per class). This alignment ensures that the synthetic samples generated by the specific GANs closely match the distribution of the real data in the training set. For example, \texttt{dcgan\_1000} achieves the highest accuracy of 0.9900 with 1000 synthetic samples per class, surpassing \texttt{dcgan\_full}’s 0.9875, because it is better tuned to the specific characteristics of the 1000 real samples per class dataset.
    
    \item \textbf{Limited Diversity in \texttt{dcgan\_full} Outputs:} Although \texttt{dcgan\_full} is trained on the entire MNIST dataset (60,000 samples), its effective training may have led it to overfit to certain digit patterns, resulting in synthetic samples that lack diversity. If \texttt{dcgan\_full} generates digits with the same shapes repeatedly, it fails to introduce the varied examples needed to improve the classifier’s generalization. This lack of diversity limits the utility of its synthetic samples for training, as seen in its lower accuracies (e.g., 0.9875 for 1000 real samples with 1000 synthetic samples) compared to \texttt{dcgan\_1000} (0.9900).
    
    \item \textbf{Dataset-Specific Variations:} The ReducedMNIST dataset is a subset of MNIST, potentially with a narrower range of variations (e.g., specific writing styles or noise levels). The specific GANs, trained on this subset, are more likely to capture these dataset-specific characteristics, producing synthetic samples that align closely with the real data distribution. In contrast, \texttt{dcgan\_full} might generate digits that include broader variations from the full MNIST dataset, which may not be as relevant to the ReducedMNIST test set, leading to a slight mismatch.
\end{itemize}

\textbf{Reevaluating the Role of \texttt{dcgan\_full}:}

While \texttt{dcgan\_full} does provide a benefit in the 300 real samples per class case—where \texttt{dcgan\_300} performs poorly (down to 0.9110) and \texttt{dcgan\_full} achieves a peak accuracy of 0.9800 with 2000 synthetic samples per class—its overall contribution to improving classifier accuracy is limited compared to the specific GANs in the 700 and 1000 real samples cases. This suggests that:

\begin{itemize}
    \item \textbf{Overfitting to Common Patterns:} The extensive training of \texttt{dcgan\_full} on the full MNIST dataset may have caused it to overfit to common digit patterns, leading to synthetic samples that are high quality but lack the diversity needed for effective augmentation. For example, if \texttt{dcgan\_full} repeatedly generates digits with similar shapes (e.g., a standardized "5" with minimal stylistic variation), the classifier does not benefit from the varied examples that would help it generalize better.
    
    \item \textbf{Benefit in Data-Scarce Scenarios:} In the 300 real samples per class case, \texttt{dcgan\_full}’s synthetic samples are still more useful than those from \texttt{dcgan\_300} because the latter is severely undertrained and produces low-quality, noisy digits. The relatively high quality of \texttt{dcgan\_full}’s samples helps maintain or slightly improve accuracy (e.g., 0.9800 vs. 0.9785 baseline), but the lack of diversity limits the extent of this improvement.
    
    \item \textbf{Diminishing Returns with Larger Real Datasets:} For 700 and 1000 real samples per class, where the specific GANs already produce high-quality and diverse synthetic samples, the lack of diversity in \texttt{dcgan\_full}’s outputs becomes a more significant drawback. The specific GANs, being better aligned with the dataset, provide more effective augmentation, as seen in their higher accuracies (e.g., 0.9860 for \texttt{dcgan\_700} vs. 0.9855 for \texttt{dcgan\_full} with 700 real samples and 1000 synthetic samples).
\end{itemize}

In summary, while \texttt{dcgan\_full} generates high-quality synthetic digits, its potential overfitting to common patterns in the full MNIST dataset may result in limited diversity, reducing its effectiveness in enhancing the classifier’s performance compared to the specific GANs. The specific GANs, tailored to the ReducedMNIST dataset, provide more diverse and relevant synthetic samples, leading to greater improvements in accuracy, especially in scenarios with sufficient real data (700 and 1000 real samples per class).
\subsection{Proposed Pipeline}
\label{subsec:pipeline}
We focus on the case of 300 real samples per class, which, with 10 classes (digits 0–9), totals 300 $\times$ 10 = 3000 real samples. The goal is to combine data augmentation and GAN-based synthetic data generation to enhance the training dataset for the ReducedMNIST classification task, and compare the best case with the baseline of 1000 real samples per class (1000 $\times$ 10 = 10,000 real samples) without augmentation or synthetic data. The proposed pipeline is as follows:

\begin{enumerate}
    \item \textbf{Initial Data Preparation}: Start with 300 real samples per class, totaling 3000 real samples across 10 classes. For each class, apply the augmentation techniques described in Problem 2 (e.g., White Noise, Random Brightness, Rotation, etc.) to generate 1000 augmented samples per class, resulting in 1000 $\times$ 10 = 10,000 augmented samples in total.
    
    \item \textbf{Train a GAN}: Use the combined dataset of 3000 real and 10,000 augmented samples (13,000 samples total) to train a Generative Adversarial Network (GAN). The GAN will consist of a Generator and Discriminator, with architectures suitable for MNIST data (e.g., Generator with transposed convolutions and Discriminator with convolutional layers). Train the GAN for a sufficient number of epochs (e.g., 100 epochs) to ensure the Generator produces realistic synthetic digits.
    
    \item \textbf{Generate Synthetic Data}: Use the trained GAN to generate an additional 1000 synthetic samples per class, resulting in 1000 $\times$ 10 = 10,000 synthetic samples in total. These synthetic samples will be labeled according to their respective classes, ensuring compatibility with the supervised classification task.
    
    \item \textbf{Combine Datasets}: Combine the 3000 real samples, 10,000 augmented samples, and 10,000 GAN-generated synthetic samples into a single training dataset, totaling 3000 + 10,000 + 10,000 = 23,000 samples.
    
    \item \textbf{Train the Classifier}: Use the combined dataset to train the CNN classifier defined in Table \ref{tab:classifier_architecture}. Train the model using the same hyperparameters as in Problem 2 (Adam optimizer, sparse categorical crossentropy loss, and accuracy metric) for a fixed number of epochs (20 epochs) to ensure consistency.
    
\end{enumerate}

This pipeline is designed to maximize training data diversity for the 300 real samples per class case by leveraging both data augmentation and GAN-based synthetic data generation. The comparison with the baseline will provide insights into the effectiveness of this approach in reducing reliance on real data.

\subsection{Results of the Pipeline}

Following the pipeline outlined in Subsection \ref{subsec:pipeline} for Problem 3, we began with 300 real samples per class, totaling 300 $\times$ 10 = 3000 real samples across the 10 digit classes in the ReducedMNIST dataset. We then applied the augmentation techniques described in Problem 2 to generate 1000 augmented samples per class, resulting in 1000 $\times$ 10 = 10,000 augmented samples. This combined dataset of 3000 real and 10,000 augmented samples (13,000 samples total) was used to train a Generative Adversarial Network (GAN).

After training the GAN for sufficient time, we utilized it to generate synthetic digit samples. Figure \ref{fig:gan_augmented_output} illustrates the output of this GAN, showcasing the generated digits.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{300_GAN_Augmented_Output.png}
    \caption{Synthetic digits generated by the GAN trained on 300 real samples per class augmented to 1000 samples per class (13,000 total samples).}
    \label{fig:gan_augmented_output}
\end{figure}

The quality of the synthetic digits generated by this GAN is notably superior to those produced by a GAN trained solely on the 300 real samples per class (without augmentation), as shown in Figure \ref{fig:gan_output_300_sub}. The augmentation step provided a more diverse and robust training set, enabling the GAN to better capture the underlying data distribution and produce more realistic digits.

Moreover, the generated digits are comparable in quality to those produced by a GAN trained on the larger dataset of 1000 real samples per class (10,000 total real samples), as depicted in Figure \ref{fig:gan_output_1000_sub}. This similarity suggests that augmenting the smaller dataset of 300 real samples per class to 13,000 samples (real + augmented) allows the GAN to achieve a performance level close to that of a GAN trained on a much larger real dataset, effectively reducing the dependency on real data while maintaining high-quality synthetic output.

The test accuracies of the classifier trained on different mixtures of real, augmented, and synthetic data are presented in Table~\ref{tab:pipeline_results}. The table shows the accuracies for the 300 real samples per class case, with 1000 augmented samples per class, combined with 0, 1000, 2000, and 3000 synthetic samples per class (0, 10,000, 20,000, and 30,000 total synthetic samples).

\begin{table}[h]
\centering
\caption{Test accuracies for the 300 real samples per class case with 1000 augmented samples per class and varying amounts of synthetic data.}
\label{tab:pipeline_results}
\small
\begin{tabular}{|p{1.5cm}|p{2cm}|p{1.5cm}|p{1.5cm}|p{1.5cm}|p{1.5cm}|}
\hline
\textbf{Real Samples} & \textbf{Augmented Samples} & \textbf{0 Gen.} & \textbf{1000 Gen.} & \textbf{2000 Gen.} & \textbf{3000 Gen.} \\
\hline
300 & 1000 & 0.9850 & 0.9855 & 0.9875 & 0.9860 \\
\hline
\end{tabular}
\end{table}

\textbf{Analysis of the Pipeline Results:}

\begin{itemize}
    \item \textbf{Effect of Synthetic Data:} In Table \ref{tab:results}, the baseline accuracy for 300 real samples per class with 0 generated samples (and no augmentation) is 0.9785. In this pipeline, after augmenting to 1000 samples per class (13,000 total samples), the baseline accuracy with 0 synthetic samples increases to 0.9850, a 0.0065 (0.65\%) improvement. This demonstrates the effectiveness of the augmentation step in enhancing the classifier’s performance even before adding synthetic data. Adding synthetic data further improves performance, with accuracies of 0.9855, 0.9875, and 0.9860 for 1000, 2000, and 3000 synthetic samples per class, respectively. The best performance is achieved with 2000 synthetic samples per class, reaching an accuracy of 0.9875, which is a 0.0025 increase over the augmented baseline (0.9850) and a 0.0090 (0.90\%) improvement over the original baseline of 0.9785. The slight drop to 0.9860 with 3000 synthetic samples suggests diminishing returns, possibly due to redundancy or minor noise in the additional synthetic data.
    
    \item \textbf{Comparison with 1000 Real Samples per Class Best Case:} Problem 3 requires comparing the best combination for 300 real samples per class with the best case for 1000 real samples per class. From Table \ref{tab:results}, the best case for 1000 real samples per class, using the specific GAN (\texttt{dcgan\_1000}) with 1000 synthetic samples per class, achieved an accuracy of 0.9900. In comparison, the best case in this pipeline (300 real samples, 1000 augmented, 2000 synthetic samples per class) achieves an accuracy of 0.9875. This result is very close, with a difference of only 0.0025 (0.25\%), demonstrating that the combination of augmentation and synthetic data generation can nearly match the performance of a much larger real dataset. This highlights the effectiveness of the pipeline in reducing the dependency on real data while maintaining high classification performance.
    
    \item \textbf{Best Case in This Pipeline:} The best performance in this pipeline is achieved with 300 real samples per class, 1000 augmented samples per class, and 2000 synthetic samples per class, yielding an accuracy of 0.9875. Compared to the original baseline of 0.9785 (from Table \ref{tab:results}), this represents a significant improvement of 0.0090 (0.90\%), underscoring the combined benefit of augmentation and synthetic data. This combination strikes an optimal balance, where the augmented data provides a robust foundation for GAN training, and the 2000 synthetic samples per class (20,000 total) add sufficient diversity to enhance the classifier’s generalization without introducing excessive noise or redundancy. The high quality of the synthetic digits, as noted earlier, contributes to this performance, making this combination a strong candidate for practical applications where real data is limited.
\end{itemize}
\section{Problem 4: Impact of Attention Mechanisms in Deep Learning}

This section explores the transformative role of attention mechanisms in enhancing neural network performance across two distinct recognition tasks: digit classification and audio recognition. The analysis is divided into two complementary parts: Part A investigates visual recognition through the Reduced MNIST dataset, while Part B examines audio-based recognition via spectrograms of spoken digits. 

\section{Part A: Understanding the Profound Impact of Attention Mechanisms on Reduced MNIST Classification}

\subsection{Introduction}
The Reduced MNIST dataset, a thoughtfully curated subset of the iconic MNIST collection, comprises grayscale images of handwritten digits (0-9), each meticulously rendered at a resolution of 28x28 pixels and organized by class. This project represents a significant evolution from prior assignments, aiming to harness the power of attention mechanisms to elevate the classification prowess of CNNs. \\
\\
This study embarks on a detailed exploration of how attention mechanisms transform the performance of Convolutional Neural Networks (CNNs) for classifying handwritten digits. Five meticulously crafted models were developed and rigorously evaluated: a foundational Base CNN, a CNN augmented with Spatial Attention, a CNN enriched with Self-Attention, a CNN fortified with the Convolutional Block Attention Module (CBAM) incorporating both channel and spatial attention, and a CNN optimized with Squeeze-and-Excitation (SE) Channel Attention. Each model incorporated a dropout rate of 0.3 to mitigate overfitting, a critical regularization strategy given the dataset's characteristics. The standout performer, the CNN with SE Channel Attention, achieved an impressive test accuracy of 0.9855, surpassing the Base CNN's 0.9810, while the Self-Attention CNN lagged at 0.9820. Training times varied significantly, with the Self-Attention model requiring a substantial 80.55 seconds due to its computational intensity.\\
\\
The primary objectives are multifaceted:
\begin{itemize}
    \item \textbf{Develop a Base CNN}: Establish a robust baseline model, building upon the foundational LeNet-inspired architecture, to serve as a reference point for performance evaluation.
    \item \textbf{Implement Diverse Attention Mechanisms}: Integrate a suite of attention strategies—Spatial Attention, Self-Attention, CBAM (channel and spatial attention), and SE Channel Attention—into CNN frameworks, each designed to refine feature focus and enhance decision-making.
    \item \textbf{Conduct Comparative Analysis}: Rigorously compare these models in terms of classification accuracy and training time, providing a quantitative assessment of their efficacy.
    \item \textbf{Visualize Attention Impact}: Leverage attention maps to offer visual insights into the regions of digit images that each model prioritizes, bridging the gap between numerical results and interpretability.\\
\end{itemize}

\newpage
\subsection{Methodology}

\subsubsection{Dataset and Preprocessing}
The Reduced MNIST dataset, a distilled version of the original MNIST collection, was meticulously loaded from directory structures, with images organized by digit class (0-9). This dataset, comprising 5000 training images and 1000 test images, was preprocessed with precision to optimize CNN performance. Each grayscale image, originally 28x28 pixels, was normalized to the range [0, 1] by dividing pixel values by 255, ensuring compatibility with neural network activation functions and promoting stable gradient flow. The images were reshaped to (28, 28, 1) to accommodate the CNN input requirements, preserving the single-channel nature of grayscale data. A random seed of 4 was employed to shuffle the dataset, guaranteeing reproducibility across experiments and maintaining a consistent evaluation framework. The resulting dataset shapes are:
\begin{itemize}
    \item \textbf{Training Images}: (5000, 28, 28, 1), representing a diverse array of handwritten digit samples.
    \item \textbf{Training Labels}: (5000,), integer-encoded class labels from 0 to 9.
    \item \textbf{Testing Images}: (1000, 28, 28, 1), a held-out set for final evaluation.
    \item \textbf{Testing Labels}: (1000,), corresponding class labels.
\end{itemize}
This preprocessing pipeline laid a solid foundation, ensuring the data was primed for the intricate feature extraction processes of the subsequent models.

\subsubsection{Network Architectures}
Five distinct CNN models were engineered using TensorFlow and Keras, each infused with a dropout rate of 0.3 after the dense layer to mitigate overfitting—a critical consideration given the dataset's moderate size and the risk of memorizing training patterns. The architectures, tailored to exploit the 28x28x1 input, are detailed below and summarized in Table \ref{tab:architectures} for clarity and comparison.

\paragraph{Base CNN}
The Base CNN serves as the foundational benchmark, drawing inspiration from the classic LeNet architecture, renowned for its success in digit classification tasks. This model adopts a straightforward yet effective design to extract hierarchical features from handwritten digits:
\begin{itemize}
    \item \textbf{Input Layer}: Accepts 28x28x1 grayscale images, capturing the raw pixel intensities of each digit.
    \item \textbf{First Convolutional Block}:
        \begin{itemize}
            \item \textbf{Conv2D}: Applies 32 filters with a 3x3 kernel and ReLU activation, reducing spatial dimensions to 26x26x32. This layer detects low-level features such as edges and corners, forming the building blocks of digit shapes.
            \item \textbf{MaxPooling2D}: Employs a 2x2 pool with stride 2, downsampling to 13x13x32, halving the spatial dimensions while retaining the most prominent features, thus reducing computational complexity.
        \end{itemize}
    \item \textbf{Second Convolutional Block}:
        \begin{itemize}
            \item \textbf{Conv2D}: Increases to 64 filters, 3x3 kernel, with ReLU, outputting 11x11x64. This layer captures more complex patterns, such as loops or intersections, essential for distinguishing digits like "8" or "0".
            \item \textbf{MaxPooling2D}: Another 2x2 pool, reducing to 5x5x64, further condensing the feature representation.
        \end{itemize}
    \item \textbf{Fully Connected Layers}:
        \begin{itemize}
            \item \textbf{Flatten}: Transforms the 5x5x64 feature map into a 1600-dimensional vector, preparing it for classification.
            \item \textbf{Dense}: 128 units with ReLU activation, serving as a hidden layer to model high-level abstractions and refine feature combinations.
            \item \textbf{Dropout}: Applies a 0.3 dropout rate, randomly deactivating 30\% of units during training to prevent co-adaptation and enhance generalization.
            \item \textbf{Dense}: 10 units with softmax activation, producing a probability distribution over the 10 digit classes.
        \end{itemize}
\end{itemize}
This architecture, with approximately 200,000 parameters, leverages a simple yet powerful structure to achieve high baseline performance, setting the stage for attention-enhanced variants.

\paragraph{CNN with Spatial Attention}
The CNN with Spatial Attention extends the base model by introducing a spatial attention mechanism, designed to emphasize spatially significant regions within the digit images:
\begin{itemize}
    \item \textbf{Input Layer}: 28x28x1 grayscale images.
    \item \textbf{First Convolutional Block}:
        \begin{itemize}
            \item \textbf{Conv2D}: 32 filters, 3x3, ReLU, outputs 26x26x32.
            \item \textbf{MaxPooling2D}: 2x2 pool, outputs 13x13x32.
        \end{itemize}
    \item \textbf{Spatial Attention}:
        \begin{itemize}
            \item \textbf{Conv2D}: 1 filter, 1x1 kernel, sigmoid activation, applied to the feature map to generate spatial weights.
            \item \textbf{Multiply}: Weights the 13x13x32 feature map, highlighting regions like digit strokes or curves (e.g., loops of '8' or the bar of '1').
        \end{itemize}
    \item \textbf{Second Convolutional Block}:
        \begin{itemize}
            \item \textbf{Conv2D}: 64 filters, 3x3, ReLU, outputs 11x11x64.
            \item \textbf{MaxPooling2D}: 2x2 pool, outputs 5x5x64.
        \end{itemize}
    \item \textbf{Fully Connected Layers}: Identical to the base model (Flatten, Dense(128), Dropout(0.3), Dense(10)).
\end{itemize}
The spatial attention adds approximately 1000 parameters, focusing the model on key spatial areas, though its impact is tempered by the dataset's simplicity.

\paragraph{CNN with Self-Attention}
The CNN with Self-Attention introduces a transformer-inspired approach, leveraging multi-head attention to model long-range dependencies within the feature maps:
\begin{itemize}
    \item \textbf{Input Layer}: 28x28x1 grayscale images.
    \item \textbf{First Convolutional Block}:
        \begin{itemize}
            \item \textbf{Conv2D}: 32 filters, 3x3, ReLU, outputs 26x26x32.
            \item \textbf{MaxPooling2D}: 2x2 pool, outputs 13x13x32.
        \end{itemize}
    \item \textbf{Second Convolutional Block}:
        \begin{itemize}
            \item \textbf{Conv2D}: 64 filters, 3x3, ReLU, outputs 11x11x64.
            \item \textbf{MaxPooling2D}: 2x2 pool, outputs 5x5x64.
        \end{itemize}
    \item \textbf{Self-Attention}:
        \begin{itemize}
            \item \textbf{Reshape}: Transforms 5x5x64 to (25, 64) for 5x5 patches, treating each patch as a token for attention computation.
            \item \textbf{MultiHeadAttention}: 4 heads, key dimension 64, captures relationships between patches across the image.
            \item \textbf{LayerNormalization}: Stabilizes the attention output, ensuring consistent feature scaling.
        \end{itemize}
    \item \textbf{Fully Connected Layers}: Same as base model.
\end{itemize}
This model adds approximately 50,000 parameters due to the attention heads, aiming to model global dependencies, though its complexity proved less effective for the compact Reduced MNIST images.

\paragraph{CNN with CBAM}
The CNN with CBAM integrates the full Convolutional Block Attention Module \cite{cbam_paper}, combining channel and spatial attention for a holistic feature refinement:
\begin{itemize}
    \item \textbf{Input Layer}: 28x28x1 grayscale images.
    \item \textbf{First Convolutional Block}:
        \begin{itemize}
            \item \textbf{Conv2D}: 32 filters, 3x3, ReLU, outputs 26x26x32.
            \item \textbf{MaxPooling2D}: 2x2 pool, outputs 13x13x32.
        \end{itemize}
    \item \textbf{CBAM}:
        \begin{itemize}
            \item \textbf{Channel Attention}: Global average pooling summarizes spatial data, followed by two dense layers (reduction ratio 8, sigmoid activation) to weight channels, emphasizing frequency-like features.
            \item \textbf{Spatial Attention}: 7x7 convolution on concatenated average/max-pooled maps generates spatial weights, highlighting regions like the vertical stroke of '1'.
            \item \textbf{Multiply}: Applies weights to refine the 13x13x32 feature map.
        \end{itemize}
    \item \textbf{Second Convolutional Block}:
        \begin{itemize}
            \item \textbf{Conv2D}: 64 filters, 3x3, ReLU, outputs 11x11x64.
            \item \textbf{MaxPooling2D}: 2x2 pool, outputs 5x5x64.
        \end{itemize}
    \item \textbf{Fully Connected Layers}: Same as base model.
\end{itemize}
CBAM adds approximately 2000 parameters, offering a dual focus that enhances feature relevance but increases computational cost.

\paragraph{CNN with SE Channel Attention}
The CNN with SE Channel Attention employs the Squeeze-and-Excitation (SE) module \cite{se_paper}, a lightweight channel attention mechanism:
\begin{itemize}
    \item \textbf{Input Layer}: 28x28x1 grayscale images.
    \item \textbf{First Convolutional Block}:
        \begin{itemize}
            \item \textbf{Conv2D}: 32 filters, 3x3, ReLU, outputs 26x26x32.
            \item \textbf{MaxPooling2D}: 2x2 pool, outputs 13x13x32.
        \end{itemize}
    \item \textbf{SE Module}:
        \begin{itemize}
            \item \textbf{Global Average Pooling}: Summarizes spatial information into a channel descriptor.
            \item \textbf{Dense}: 4 units, ReLU, reduces dimensionality for efficiency.
            \item \textbf{Dense}: 32 units, sigmoid, generates channel weights.
            \item \textbf{Scale}: Applies weights to refine the 13x13x32 feature map, emphasizing discriminative features like edges.
        \end{itemize}
    \item \textbf{Second Convolutional Block}:
        \begin{itemize}
            \item \textbf{Conv2D}: 64 filters, 3x3, ReLU, outputs 11x11x64.
            \item \textbf{MaxPooling2D}: 2x2 pool, outputs 5x5x64.
        \end{itemize}
    \item \textbf{Fully Connected Layers}: Same as base model.
\end{itemize}
The SE module adds approximately 500 parameters, offering an efficient channel weighting strategy that proved highly effective.

\begin{table}[H]
    \centering
    \caption{Part 1: Network Architectures for Reduced MNIST Classification Models (Base and Spatial Attention)}
    \label{tab:architectures_part1}
    \begin{tabularx}{\textwidth}{l X}
        \toprule
        \textbf{Model} & \textbf{Architecture} \\
        \midrule
        Base CNN & 
        \begin{itemize}
            \item \textbf{Input Layer}: 28x28x1 grayscale images.
            \item \textbf{First Convolutional Block}: Conv2D(32, 3$\times$3, ReLU) $\rightarrow$ MaxPooling2D(2$\times$2) to 13x13x32, detecting edges and basic shapes.
            \item \textbf{Second Convolutional Block}: Conv2D(64, 3$\times$3, ReLU) $\rightarrow$ MaxPooling2D(2$\times$2) to 5x5x64, capturing complex patterns like loops.
            \item \textbf{Fully Connected Layers}: Flatten to 1600 $\rightarrow$ Dense(128, ReLU) $\rightarrow$ Dropout(0.3) $\rightarrow$ Dense(10, Softmax).
        \end{itemize} \\
        \midrule
        CNN with Spatial Attention & 
        \begin{itemize}
            \item \textbf{Input Layer}: 28x28x1 grayscale images.
            \item \textbf{First Convolutional Block}: Conv2D(32, 3$\times$3, ReLU) $\rightarrow$ MaxPooling2D(2$\times$2) to 13x13x32.
            \item \textbf{Spatial Attention}: Conv2D(1, 1$\times$1, Sigmoid) $\rightarrow$ Multiply with feature maps, emphasizing digit strokes (e.g., '8' loops).
            \item \textbf{Second Convolutional Block}: Conv2D(64, 3$\times$3, ReLU) $\rightarrow$ MaxPooling2D(2$\times$2) to 5x5x64.
            \item \textbf{Fully Connected Layers}: Same as base model.
        \end{itemize} \\
        \bottomrule
    \end{tabularx}
\end{table}

\begin{table}[H]
    \centering
    \caption{Part 2: Network Architectures for Reduced MNIST Classification Models (Self-Attention, CBAM, and SE Channel Attention)}
    \label{tab:architectures_part2}
    \begin{tabularx}{\textwidth}{l X}
        \toprule
        \textbf{Model} & \textbf{Architecture} \\
        \midrule
        CNN with Self-Attention & 
        \begin{itemize}
            \item \textbf{Input Layer}: 28x28x1 grayscale images.
            \item \textbf{First Convolutional Block}: Conv2D(32, 3$\times$3, ReLU) $\rightarrow$ MaxPooling2D(2$\times$2) to 13x13x32.
            \item \textbf{Second Convolutional Block}: Conv2D(64, 3$\times$3, ReLU) $\rightarrow$ MaxPooling2D(2$\times$2) to 5x5x64.
            \item \textbf{Self-Attention}: Reshape to (25, 64) for 5$\times$5 patches $\rightarrow$ MultiHeadAttention(4 heads, key dim=64) $\rightarrow$ LayerNormalization, modeling global dependencies.
            \item \textbf{Fully Connected Layers}: Same as base model.
        \end{itemize} \\
        \midrule
        CNN with CBAM & 
        \begin{itemize}
            \item \textbf{Input Layer}: 28x28x1 grayscale images.
            \item \textbf{First Convolutional Block}: Conv2D(32, 3$\times$3, ReLU) $\rightarrow$ MaxPooling2D(2$\times$2) to 13x13x32.
            \item \textbf{CBAM}: Channel Attention (Global Pooling + MLP) weights frequency features $\rightarrow$ Spatial Attention (7x7 Conv) highlights spatial regions (e.g., '1' stroke).
            \item \textbf{Second Convolutional Block}: Conv2D(64, 3$\times$3, ReLU) $\rightarrow$ MaxPooling2D(2$\times$2) to 5x5x64.
            \item \textbf{Fully Connected Layers}: Same as base model.
        \end{itemize} \\
        \midrule
        CNN with SE Channel Attention & 
        \begin{itemize}
            \item \textbf{Input Layer}: 28x28x1 grayscale images.
            \item \textbf{First Convolutional Block}: Conv2D(32, 3$\times$3, ReLU) $\rightarrow$ MaxPooling2D(2$\times$2) to 13x13x32.
            \item \textbf{SE Module}: Global Average Pooling $\rightarrow$ Dense(4, ReLU) $\rightarrow$ Dense(32, Sigmoid) $\rightarrow$ Scale, reweighting channels for key features (e.g., edges).
            \item \textbf{Second Convolutional Block}: Conv2D(64, 3$\times$3, ReLU) $\rightarrow$ MaxPooling2D(2$\times$2) to 5x5x64.
            \item \textbf{Fully Connected Layers}: Same as base model.
        \end{itemize} \\
        \bottomrule
    \end{tabularx}
\end{table}

\subsubsection{Training Process and Hyperparameters}
All models underwent a standardized training regimen to ensure a fair and equitable comparison, with hyperparameters meticulously chosen to optimize performance while addressing potential pitfalls. The details are encapsulated in Table \ref{tab:hyperparameters}, and the process is elaborated below to provide a comprehensive understanding.

\paragraph{Training Procedure}
The training was conducted in a controlled environment, enforcing CPU computation by setting the \texttt{CUDA\_VISIBLE\_DEVICES} environment variable to an empty string. This decision ensured consistency across models, though it sacrificed speed for reliable benchmarking, a trade-off deemed acceptable given the assignment's focus on comparison. Training times were meticulously recorded using Python's \texttt{time} module, capturing the duration from the first epoch to the last. The test set, comprising 1000 images, doubled as the validation set to monitor performance dynamically during training, providing immediate feedback on generalization and enabling real-time adjustments. Each model was trained for 10 epochs, a duration determined through preliminary runs where convergence was typically achieved within 8-10 epochs, balancing thorough learning with computational efficiency.

\paragraph{Hyperparameter Selection}
The hyperparameter configuration was a result of iterative experimentation, balancing model capacity, regularization, and computational efficiency:
\begin{itemize}
    \item \textbf{Optimizer}: The Adam optimizer was selected for its adaptive learning rate properties, utilizing default settings to ensure broad applicability across models. This choice facilitated stable convergence without requiring extensive tuning, allowing the models to adapt to the varying complexities introduced by attention mechanisms.
    \item \textbf{Loss Function}: Sparse categorical crossentropy was employed, aligning perfectly with the multi-class classification task. This loss function computes the cross-entropy loss between true integer labels and predicted probability distributions, providing a robust metric for backpropagation across the 10 digit classes, ensuring accurate error propagation.
    \item \textbf{Epochs}: Set to 10, this number was chosen based on initial trials where models converged within 8-10 epochs, avoiding unnecessary computation while ensuring thorough learning. The fixed epoch count allowed direct comparison, though the incorporation of early stopping in future iterations could optimize this further.
    \item \textbf{Batch Size}: A batch size of 32 was adopted, striking a balance between memory utilization and gradient update frequency. This size ensured that the models could process sufficient samples per iteration on the CPU, maintaining stability without overwhelming resources, and provided a robust gradient estimate for optimization.
    \item \textbf{Regularization}: A uniform dropout rate of 0.3 was applied after the Dense(128) layer across all models. This regularization technique randomly deactivates 30\% of neurons during training, preventing co-adaptation of features and enhancing generalization. The consistency across models ensured that any performance differences were attributable to the attention mechanisms rather than regularization disparities, a deliberate design choice to isolate their impact.
    \item \textbf{Random Seed}: Fixed at 4 for reproducibility, this seed ensured that shuffling and data splits remained consistent, allowing for reliable comparisons across experimental runs and eliminating variability as a confounding factor.
\end{itemize}

\begin{table}[H]
    \centering
    \caption{Training Hyperparameters for All Models}
    \label{tab:hyperparameters}
    \begin{tabular}{lc}
        \toprule
        \textbf{Hyperparameter} & \textbf{Value} \\
        \midrule
        Optimizer & Adam (default learning rate) \\
        Loss Function & Sparse Categorical Crossentropy \\
        Epochs & 10 \\
        Batch Size & 32 \\
        Validation Data & Test set (1000 images) \\
        Random Seed & 4 \\
        Regularization & Dropout (0.3) after Dense(128) \\
        \bottomrule
    \end{tabular}
\end{table}

\subsubsection{Training Challenges and Solutions}
The training process encountered several challenges that shaped its evolution. Initially, models without regularization exhibited pronounced overfitting, with training accuracy soaring to 99\% while validation accuracy plateaued around 97\%, a clear sign of memorization over generalization. The introduction of dropout (0.3) effectively bridged this gap, stabilizing performance across all models by introducing stochasticity and preventing co-adaptation of neurons. Additionally, the computational intensity of attention mechanisms, particularly the self-attention model with its multi-head architecture, posed a significant challenge, necessitating careful monitoring of training times to assess their feasibility. The decision to enforce CPU computation ensured consistency across runs, though it limited speed, a trade-off accepted to prioritize reliable benchmarking over rapid execution. These challenges were met with thoughtful solutions—regularization tuning, consistent hyperparameter application, and meticulous time tracking—ensuring a robust training framework that supported the subsequent comparative analysis.

\subsection{Results}

\subsubsection{Performance Comparison}
The performance of the five models is meticulously summarized in Table \ref{tab:performance}, providing a quantitative snapshot of their efficacy in terms of test accuracy and training time. To offer a dynamic perspective, accuracy curves are visualized in Figures \ref{fig:accuracy_curves_base_spatial}, \ref{fig:accuracy_curve_self}, \ref{fig:accuracy_curve_cbam}, and \ref{fig:accuracy_curve_se}, capturing the evolution of model learning over the 10 epochs. These curves illuminate the learning trajectories, highlighting convergence rates and the stabilizing influence of dropout. Attention maps, generated to illuminate the models' focus, are presented in Figures \ref{fig:spatial_attention_maps}, \ref{fig:self_attention_maps}, and \ref{fig:cbam_attention_maps}, offering a visual testament to the attention mechanisms' impact and providing a bridge between numerical results and interpretability.

\begin{table}[H]
    \centering
    \caption{Performance Comparison of Models}
    \label{tab:performance}
    \begin{tabular}{l c c c}
        \toprule
        \textbf{Model} & \textbf{Test Accuracy} & \textbf{Training Time (s)} & \textbf{ Time Difference (s)} \\
        \midrule
        Base CNN & 0.9810 & 41.41 & -- \\
        CNN with Spatial Attention & 0.9790 & 48.30 & 6.90 (vs. Base) \\
        CNN with Self-Attention & 0.9820 & 80.55 & 39.14 (vs. Base) \\
        CNN with CBAM & 0.9830 & 63.40 & 21.99 (vs. Base) \\
        CNN with SE Channel Attention & 0.9855 & 44.61 & 3.20 (vs. Base) \\
        \bottomrule
    \end{tabular}
\end{table}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{accuracy_curves_base_spatial.png}
    \caption{Accuracy Curves for Base CNN and CNN with Spatial Attention over 10 Epochs. The Base CNN rises steadily to 0.9810, reflecting its robust feature extraction, while the Spatial Attention model peaks at 0.9790, showing a slight dip due to marginal attention benefits. The curves exhibit a tight train-validation gap, a testament to dropout's effectiveness in stabilizing generalization on this simple dataset.}
    \label{fig:accuracy_curves_base_spatial}
\end{figure}
\paragraph{Comment}:The Base CNN's consistent performance highlights its adequacy for Reduced MNIST, while the Spatial Attention model's slight decline suggests that spatial focus adds minimal value for centered digits, a trend mitigated by dropout.
\paragraph{Conclusion}:The similarity in curves indicates that spatial attention's complexity may not justify its use here, though it remains a viable approach for more varied datasets.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{accuracy_curve_self_attention.png}
    \caption{Accuracy Curve for CNN with Self-Attention over 10 Epochs. The curve ascends to 0.9820, with a noticeable train-validation gap reduced by dropout, indicating that self-attention struggles to leverage long-range dependencies in this small, low-resolution dataset.}
    \label{fig:accuracy_curve_self}
\end{figure}
\paragraph{Comment}: The Self-Attention model's 0.9820 accuracy, despite a significant 80.55-second training time, suggests that modeling global dependencies is computationally expensive but offers limited benefit for 28x28 images with centered digits. Dropout helps, but the gap persists.
\paragraph{Conclusion}: The high computational cost and modest accuracy gain indicate that self-attention is less suited to Reduced MNIST, warranting exploration on larger or more complex datasets.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{accuracy_curve_cbam.png}
    \caption{Accuracy Curve for CNN with CBAM over 10 Epochs. The curve reaches 0.9830, showing a balanced rise with dropout smoothing the train-validation gap, though the dual attention mechanism's complexity yields only a slight improvement over the base model.}
    \label{fig:accuracy_curve_cbam}

\end{figure}
\paragraph{Comment}: The CBAM model's 0.9830 accuracy, with a 63.40-second training time, reflects the benefit of its dual channel-spatial focus, as seen in attention maps. However, the modest 0.002 gain over the base model suggests that the added complexity is not fully leveraged by the dataset's simplicity.
\paragraph{Conclusion}: CBAM's performance indicates potential for complex datasets, but its overhead may be unnecessary here, with dropout ensuring stability rather than significant enhancement.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{accuracy_curve_se.png}
    \caption{Accuracy Curve for CNN with SE Channel Attention over 10 Epochs. The curve peaks at 0.9855, with a tight train-validation gap, highlighting SE's efficiency and effectiveness in feature reweighting for this task.}
    \label{fig:accuracy_curve_se}

\end{figure}
\paragraph{Comment}: The SE Channel Attention model's 0.9855 accuracy, achieved in 44.61 seconds, showcases its efficiency, with the tight gap reflecting dropout's role. The attention mechanism's focus on channel importance aligns perfectly with the dataset's needs.
\paragraph{Conclusion}: SE's success confirms that channel attention is optimal for Reduced MNIST, offering a lightweight, effective solution that outperforms other attention types.

\subsubsection{Attention Map Visualizations}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{spatial_attention_maps.png}
    \caption{Spatial Attention Maps for Three Test Images (Digits 1 and 9) from the CNN with Spatial Attention. The highlighted regions demonstrate effective spatial focus, though the simplicity of Reduced MNIST limits its advantage.}
    \label{fig:spatial_attention_maps}
    
\end{figure}
\paragraph{Comment}: These maps reveal that spatial attention targets digit strokes accurately, but the centered nature of Reduced MNIST digits means the base CNN already captures these regions, reducing the mechanism's impact. The 6.90-second time increase aligns with this marginal benefit.
\paragraph{Conclusion}: The spatial focus is visually compelling but offers little practical gain, suggesting its value lies in datasets with more spatial variability.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{self_attention_maps.png}
    \caption{Self-Attention Focus for Two Test Images (Digits 8 and 9) from the CNN with Self-Attention. The distributed focus across 5x5 patches suggests an attempt to model relationships, but the small image size reduces its effectiveness.}
    \label{fig:self_attention_maps}

\end{figure}
    \paragraph{Comment}: The self-attention maps show a broad focus, attempting to link patches, but the 28x28 resolution and centered digits limit its utility, contributing to the 39.14-second time increase and 0.001 accuracy drop from the base model.
    \paragraph{Conclusion}: Self-attention's complexity is better suited to larger, more complex images, where patch relationships are more pronounced.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{cbam_attention_maps.png}
    \caption{CBAM Spatial Attention Map for Two Test Images (Digits 2 and 3) from the CNN with CBAM. The map emphasizes the strokes, showcasing CBAM's dual attention capability, though its complexity didn't yield proportional accuracy gains.}
    \label{fig:cbam_attention_maps}

\end{figure}
    \paragraph{Comment}: The CBAM map's focus on the strokes reflects its dual channel-spatial strength, but the 21.99-second time increase and 0.002 accuracy gain over the base model suggest that the dataset's simplicity limits its advantage. Dropout stabilizes the curve.
    \paragraph{Conclusion}: CBAM's potential shines in more challenging datasets, where its comprehensive focus could drive larger gains.

\subsection{Analysis}

\subsubsection{Impact of Attention Mechanisms}
The comparative performance of the five models offers a rich tapestry of insights into the efficacy of attention mechanisms on the Reduced MNIST dataset. Each model, regularized with a dropout rate of 0.3, was evaluated for its test accuracy and training time, with the following detailed observations:

\begin{itemize}
    \item \textbf{Base CNN}: Achieved a test accuracy of 0.9810 with a training time of 41.41 seconds, establishing a formidable benchmark. This model, devoid of attention, relies on its convolutional and pooling layers to extract hierarchical features from the 28x28 grayscale images. The high accuracy reflects the dataset's simplicity—centered digits with minimal noise—allowing a standard CNN to capture essential patterns effectively. The accuracy curve (Figure \ref{fig:accuracy_curves_base_spatial}) shows a steady ascent, with dropout smoothing the train-validation gap, indicating robust generalization and setting a high bar for attention-enhanced models.
    
    \item \textbf{CNN with Spatial Attention}: Recorded a test accuracy of 0.9790, a modest 0.002 decrease from the base model, with a training time of 48.30 seconds (6.90 seconds longer). The spatial attention mechanism, implemented via a 1x1 convolution with sigmoid activation, weights spatial regions to emphasize digit strokes (e.g., loops of '8' or the bar of '4', as seen in Figure \ref{fig:spatial_attention_maps}). However, the Reduced MNIST dataset's centered digits and low resolution (28x28) mean that a standard CNN already focuses on these regions naturally. The added complexity introduced a slight overfitting risk, mitigated by dropout, but the accuracy drop suggests limited benefit, as confirmed by the minor train-validation gap in Figure \ref{fig:accuracy_curves_base_spatial}. The 6.90-second time increase reflects the additional computation, though it remains modest.
    
    \item \textbf{CNN with Self-Attention}: Exhibited a test accuracy of 0.9820, a 0.001 improvement over the base model, with a substantial training time of 80.55 seconds (39.14 seconds longer). The self-attention mechanism, using MultiHeadAttention with 4 heads over 5x5 patches, aims to model long-range dependencies within the feature map. The attention focus maps (Figure \ref{fig:self_attention_maps}) show a distributed emphasis across patches for digits '4' and '9', suggesting an attempt to capture global relationships. However, the small image size and simple digit structures render these dependencies less relevant, leading to a modest accuracy gain despite the significant computational cost. The accuracy curve (Figure \ref{fig:accuracy_curve_self}) reveals a larger train-validation gap, reduced by dropout but still indicating some overfitting, compounded by the mechanism's high overhead.
    
    \item \textbf{CNN with CBAM}: Attained a test accuracy of 0.9830, a 0.002 improvement over the base model, with a training time of 63.40 seconds (21.99 seconds longer). CBAM's dual approach—channel attention weighting frequency features and spatial attention highlighting regions (e.g., the '1' stroke in Figure \ref{fig:cbam_attention_maps})—offers a comprehensive feature refinement strategy. The spatial attention map demonstrates effective stroke focus, but the dataset's simplicity limits the added value, as the base CNN already captures these patterns. The accuracy curve (Figure \ref{fig:accuracy_curve_cbam}) shows a balanced rise, with dropout minimizing overfitting, though the complex attention mechanism's overhead didn't translate to proportional accuracy gains. The 21.99-second time increase reflects CBAM's dual computation.
    
    \item \textbf{CNN with SE Channel Attention}: Emerged as the top performer with a test accuracy of 0.9855, a 0.0045 improvement over the base model, and a training time of 44.61 seconds (3.20 seconds longer). The SE module, a lightweight channel attention mechanism, uses global average pooling and dense layers to reweight feature channels, emphasizing discriminative elements (e.g., edges or curves critical for '0' or '3'). This efficiency proved ideal for Reduced MNIST, avoiding the spatial complexity of other methods. The accuracy curve (Figure \ref{fig:accuracy_curve_se}) exhibits a tight train-validation gap, underscoring SE's effectiveness and dropout's role in generalization. The 3.20-second time increase is negligible, highlighting SE's practicality.
\end{itemize}

\subsubsection{Dataset Influence}
The Reduced MNIST dataset's characteristics profoundly shaped the results, offering a unique lens through which to evaluate attention mechanisms:
\begin{itemize}
    \item \textbf{Small Size}: With only 5000 training samples, the dataset's limited diversity constrained the capacity of complex models like self-attention to generalize fully. Dropout mitigated overfitting, as seen in the reduced train-validation gaps, but the small sample size favored simpler architectures like SE.
    \item \textbf{Simplicity}: The centered digits, devoid of significant background noise or occlusion, allowed the base CNN to perform admirably (0.9810). Attention mechanisms, designed to focus on specific regions or relationships, provided marginal benefits, as the dataset's straightforward nature reduced the need for such refinements.
    \item \textbf{Low Resolution}: At 28x28 pixels, the images offer limited spatial detail, making channel attention (e.g., SE) more effective than spatial or long-range dependency modeling. The attention maps (Figures \ref{fig:spatial_attention_maps}, \ref{fig:cbam_attention_maps}) highlight this, with spatial focus being redundant for centered digits.
\end{itemize}

\subsubsection{Expected vs. Actual Outcomes}
The experimental journey offered a fascinating comparison between anticipated and realized results:
\begin{itemize}
    \item \textbf{Expected Outcomes}: Attention mechanisms were anticipated to enhance accuracy by focusing on relevant features or relationships. The SE Channel Attention was expected to excel by reweighting discriminative channels, leveraging its efficiency. CBAM's hybrid approach was poised to balance feature and spatial focus, potentially outperforming spatial attention alone. Self-attention was hypothesized to capture long-range dependencies, boosting performance on complex patterns. Dropout was expected to reduce overfitting across all models, ensuring generalization.
    \item \textbf{Actual Outcomes}: The SE CNN's triumph at 0.9855 validated the hypothesis that channel attention is optimal, its lightweight design suiting the dataset's simplicity. The base CNN's strong 0.9810 performance surprised, likely due to its effective feature extraction on centered digits. Spatial, self-attention, and CBAM models underperformed (0.9790, 0.9820, 0.9830), as their added complexity didn't yield significant gains, reflecting the dataset's limited need for advanced focus. Dropout successfully reduced train-validation gaps, aligning with expectations and enhancing generalization across all models.
\end{itemize}

\subsection{Insights and Observations}
The experimental odyssey yielded a treasure trove of insights, each shedding light on the intricate dance between model design, dataset characteristics, and training dynamics:
\begin{itemize}
    \item \textbf{Channel Attention's Effectiveness}: The SE Channel Attention's leading accuracy (0.9855) underscored its prowess in reweighting feature channels, emphasizing edges and curves critical for digit identity. This finding suggests that, for Reduced MNIST, enhancing channel importance is more impactful than spatial or relational modeling, a revelation that could guide future attention designs for similar low-resolution, centered datasets.
    \item \textbf{Interpretability Through Attention Maps}: The attention maps (Figures \ref{fig:spatial_attention_maps}, \ref{fig:self_attention_maps}, \ref{fig:cbam_attention_maps}) offered a window into the models' decision-making processes. The CBAM map's focus on the '1' stroke, the spatial map's highlight of '8' loops, and the self-attention map's distributed focus provided concrete evidence of effective region targeting or its limitations, enriching our understanding of attention's practical utility and prompting deeper analysis.
    \item \textbf{Computational Cost as a Trade-Off}: The self-attention model's training time of 80.55 seconds and CBAM's 63.40 seconds contrasted sharply with the SE model's 44.61 seconds and the base model's 41.41 seconds. This disparity illuminated the computational burden of complex attention mechanisms, suggesting that efficiency gains (e.g., SE's lightweight design) can rival or surpass accuracy benefits on simple datasets, a critical consideration for resource-constrained settings and a guiding principle for future optimizations.
    \item \textbf{Regularization Impact}: The uniform dropout rate of 0.3 proved a linchpin in reducing overfitting, as evidenced by the smaller train-validation accuracy gaps in all models' curves (Figures \ref{fig:accuracy_curves_base_spatial} to \ref{fig:accuracy_curve_se}). This regularization strategy ensured that the models generalized well, a testament to its effectiveness in managing the dataset's moderate size and the added complexity of attention layers, offering a robust defense against memorization.
\end{itemize}

\subsection{Suggestions for Future Improvements}
The insights gleaned from this study open a vista of opportunities for future enhancements, each designed to refine and elevate the models' capabilities:
\begin{itemize}
    \item \textbf{Hybrid Attention Variants}: A fusion of SE with spatial attention, perhaps a streamlined CBAM variant, could harness the strengths of both channel and spatial focus. This hybrid approach might elevate accuracy beyond 0.9855 by balancing feature reweighting with spatial refinement, with attention maps guiding the integration process and revealing synergistic effects.
    \item \textbf{Advanced Regularization Techniques}: Beyond dropout, exploring weight decay or batch normalization alongside the current strategy could further reduce overfitting. Batch normalization might stabilize training for attention layers, potentially narrowing the train-validation gap observed in self-attention models, offering a new layer of robustness.
    \item \textbf{Hyperparameter Tuning}: The current hyperparameters (e.g., dropout 0.3, default Adam learning rate) offer a solid foundation, but a systematic tuning could optimize performance. Adjusting the SE reduction ratio (e.g., from 8 to 16), the number of self-attention heads (e.g., 2 to 8), or exploring learning rates (e.g., 0.001 to 0.0001) could uncover configurations that maximize accuracy and minimize training time, with attention maps providing visual feedback.
    \item \textbf{Data Augmentation Strategies}: Introducing transformations such as random rotations (±10°), slight scaling, or elastic distortions could enrich the dataset's diversity, challenging the models to generalize beyond centered digits. These augmentations might amplify the effectiveness of spatial and self-attention, with attention maps revealing their impact on focus areas and driving improved performance.
    \item \textbf{Exploration with Larger Datasets}: Testing on the full MNIST dataset (60,000 training samples) could better leverage the capacity of attention-based models. The increased diversity and volume might justify the complexity of self-attention and CBAM, potentially pushing accuracy toward 99.5\%, with attention maps providing deeper insights into feature utilization across a broader range of digit variations.
\end{itemize}
\newpage
\subsection{Conclusion for Part A}
This study explored how attention mechanisms impact CNN performance on the Reduced MNIST classification task, revealing some key insights. The CNN with SE Channel Attention gave the best results, reaching a test accuracy of 0.9855 with a short training time of 44.61 seconds. It slightly outperformed the Base CNN, which scored 0.9810. Models using Spatial Attention, Self-Attention, and CBAM had accuracies of 0.9790, 0.9820, and 0.9830 respectively. These results showed that adding more complex attention mechanisms doesn’t always help on a simple dataset. However, their attention maps did give helpful visual explanations.\\
The Self-Attention model took the longest to train at 80.55 seconds, showing that capturing long-range dependencies is computationally expensive. CBAM, which combines spatial and channel attention, also had a higher training time of 63.40 seconds. The Spatial Attention model trained in 48.30 seconds but performed slightly worse than the Base CNN, with a 0.006 drop in accuracy, suggesting limited gains from spatial attention in this case.\\
Dropout regularization (set to 0.3) was important for all models. It helped reduce overfitting and improved generalization, as shown by the smoother accuracy curves. Overall, the results highlighted that the Reduced MNIST dataset is simple enough that SE Channel Attention gives the best performance with low cost, while more advanced methods add complexity without much benefit.The attention maps backed this up by visually showing where each model focused and confirmed that SE was the most efficient.

\section{Part B: Spoken Digit Recognition with Convolutional Neural Networks: An In-Depth Analysis of Attention Mechanisms}

\subsection{Introduction}
Spoken digit recognition is a cornerstone of audio signal processing, with widespread applications in voice-activated systems, automated call centers, and human-computer interaction interfaces. This part of the assignment builds upon the insights gained from Part A by extending the exploration of attention mechanisms from visual to audio recognition tasks.\\ \\
This section presents an extensive investigation into spoken digit recognition using convolutional neural networks (CNNs), with a focus on the integration and impact of attention mechanisms. Three distinct models were developed and rigorously evaluated: a Baseline CNN based on the LeNet architecture from Assignment 2, an enhanced CNN with a full Convolutional Block Attention Module (CBAM), and a novel Hybrid Attention CNN featuring a simplified channel attention mechanism combined with noise injection augmentation. The free-spoken-digits dataset \cite{free_spoken_digits}, comprising approximately 3000 spectrogram images of spoken digits (0-9), served as the foundation for this study. Creative enhancements, including noise augmentation and a tailored attention approach, were implemented to boost model robustness and stability. Trained on a Kaggle T4 GPU with meticulous hyperparameter tuning, the models achieved remarkable test accuracies of 97.27\%, 99.22\%, and 98.44\% for the Baseline CNN, CNN with Attention, and Hybrid Attention models, respectively. The attention mechanisms significantly enhanced accuracy by focusing on critical spectrogram features, albeit at the cost of increased training time, with the full attention model taking 2.5 times longer than the baseline.\\ \\ \\ \\
Three models were meticulously designed and implemented:
\begin{itemize}
    \item \textbf{Baseline CNN}: A foundational model replicating the LeNet architecture from Assignment 2, serving as a reference point.
    \item \textbf{CNN with Attention}: An advanced model incorporating the full CBAM, which combines channel and spatial attention to refine feature focus.
    \item \textbf{Hybrid Attention CNN}: A creative innovation featuring a simplified channel attention mechanism, augmented with noise injection to improve robustness, tailored to balance performance and computational efficiency.\\
\end{itemize}
This section provides an exhaustive exploration of the network architectures, training methodologies, and performance metrics, culminating in a detailed comparison of models with and without attention. It delves into the profound impact of attention mechanisms on accuracy and training time, offers rich insights gleaned from experimental observations, and proposes forward-thinking suggestions for future enhancements. A key addition is the visualization of attention maps, which illuminate the regions of spectrograms the models prioritize, providing a visual testament to the attention mechanisms' effectiveness.

\subsection{Network Architectures}
The architectures of the three models were meticulously crafted to process spectrogram images resized to 128x128x3 pixels, classifying them into one of 10 digit classes (0-9). Each model was designed with a specific purpose: the Baseline CNN as a benchmark, the CNN with Attention to maximize feature focus, and the Hybrid Attention CNN to introduce a novel, stable enhancement. Below, I provide a detailed breakdown of each architecture, followed by a comprehensive table (Table \ref{tab:architectures_partb}) for clarity.

\subsubsection{Baseline CNN}
The Baseline CNN, inherited from Assignment 2, is inspired by the classic LeNet architecture, a pioneer in digit recognition tasks. This model serves as the foundation, relying on straightforward convolutional and pooling layers to extract features from spectrograms. Its structure is as follows:
\begin{itemize}
    \item \textbf{Input Layer}: Accepts 128x128x3 RGB spectrogram images, where each channel captures different frequency or intensity aspects.
    \item \textbf{First Convolutional Block}:
        \begin{itemize}
            \item \textbf{Conv2D}: Employs 6 filters with a 5x5 kernel, followed by ReLU activation, reducing spatial dimensions to 124x124x6. This initial layer detects basic patterns like edges or frequency bands.
            \item \textbf{AveragePooling2D}: Applies a 2x2 pool with stride 2, downsampling to 62x62x6, reducing computational load while retaining essential features.
        \end{itemize}
    \item \textbf{Second Convolutional Block}:
        \begin{itemize}
            \item \textbf{Conv2D}: Increases to 16 filters, 5x5 kernel, with ReLU, outputting 58x58x16. This layer captures more complex patterns, such as texture or frequency modulations.
            \item \textbf{AveragePooling2D}: Another 2x2 pool, reducing to 29x29x16, further condensing the feature map.
        \end{itemize}
    \item \textbf{Fully Connected Layers}:
        \begin{itemize}
            \item \textbf{Flatten}: Transforms the 29x29x16 feature map into a 13,456-dimensional vector, preparing it for classification.
            \item \textbf{Dense}: 120 units with ReLU activation, serving as a hidden layer to model higher-level abstractions.
            \item \textbf{Dense}: 84 units with ReLU, refining the feature representation.
            \item \textbf{Dense}: 10 units with softmax activation, producing probability distributions over the 10 digit classes.
        \end{itemize}
\end{itemize}
The total parameter count is approximately 1.62 million, with the dense layers dominating due to the large flattened input, reflecting a straightforward yet effective design for digit recognition.

\subsubsection{CNN with Attention}
The CNN with Attention builds upon the baseline by integrating the Convolutional Block Attention Module (CBAM) \cite{cbam_paper}, a sophisticated attention mechanism that enhances feature extraction. CBAM consists of two sub-modules:
\begin{itemize}
    \item \textbf{Channel Attention}: Utilizes global average pooling to summarize spatial information, followed by two dense layers (reduction ratio of 8, sigmoid activation) to generate channel-wise weights, emphasizing important frequency bands.
    \item \textbf{Spatial Attention}: Applies a 7x7 convolution to concatenated average- and max-pooled feature maps, producing spatial weights to highlight key time-frequency regions.
\end{itemize}
The architecture is an enriched version of the baseline:
\begin{itemize}
    \item \textbf{Input Layer}: 128x128x3 spectrograms.
    \item \textbf{First Convolutional Block}:
        \begin{itemize}
            \item \textbf{Conv2D}: 6 filters, 5x5, ReLU activation.
            \item \textbf{BatchNormalization}: Normalizes activations to stabilize training.
            \item \textbf{CBAM}: Applies channel and spatial attention to refine feature maps.
            \item \textbf{AveragePooling2D}: 2x2 pool, outputs 62x62x6.
        \end{itemize}
    \item \textbf{Second Convolutional Block}: Identical structure with 16 filters, outputting 29x29x16.
    \item \textbf{Fully Connected Layers}: Same as the baseline (120, 84, 10 units).
\end{itemize}
The addition of CBAM increases the parameter count to approximately 1.63 million, with the attention modules contributing around 330 parameters, reflecting the added complexity for improved feature focus.

\subsubsection{Hybrid Attention CNN}
The Hybrid Attention CNN represents a creative leap, introducing a simplified channel attention mechanism and noise injection augmentation to balance performance and stability. This model was developed to address the numerical instability observed with the full CBAM while retaining attention benefits:
\begin{itemize}
    \item \textbf{Input Layer}: 128x128x3 spectrograms.
    \item \textbf{First Convolutional Block}:
        \begin{itemize}
            \item \textbf{Conv2D}: 6 filters, 5x5, ReLU activation.
            \item \textbf{BatchNormalization}: Stabilizes training by normalizing activations.
            \item \textbf{Channel Attention}: Employs global average pooling, two dense layers (reduction ratio 8, sigmoid activation), and LayerNormalization to weight frequency bands, enhancing feature relevance.
            \item \textbf{Dropout}: 0.1 rate to prevent overfitting after attention.
            \item \textbf{AveragePooling2D}: 2x2 pool, outputs 62x62x6.
        \end{itemize}
    \item \textbf{Second Convolutional Block}: Repeats the above with 16 filters, outputting 29x29x16.
    \item \textbf{Fully Connected Layers}:
        \begin{itemize}
            \item \textbf{Flatten}: Converts to 13,456 features.
            \item \textbf{Dense}: 120 units, ReLU, with L2 regularization (0.001) to penalize large weights.
            \item \textbf{Dropout}: 0.5 rate to mitigate overfitting.
            \item \textbf{Dense}: 84 units, ReLU, with L2 regularization (0.001).
            \item \textbf{Dropout}: 0.5 rate.
            \item \textbf{Dense}: 10 units, softmax for classification.
        \end{itemize}
\end{itemize}
The total parameter count is 1,629,144, with the simplified attention adding minimal overhead, validated by the model summary provided earlier.

\begin{table}[h]
    \centering
    \caption{Network Architectures for Spoken Digit Recognition Models}
    \label{tab:architectures_partb}
    \begin{tabular}{lccc}
        \toprule
        \textbf{Layer Type} & \textbf{Baseline CNN} & \textbf{CNN with Attention} & \textbf{Hybrid Attention} \\
        \midrule
        Input Shape & 128x128x3 & 128x128x3 & 128x128x3 \\
        Conv2D (1st) & 6 filters, 5x5, ReLU & 6 filters, 5x5, ReLU & 6 filters, 5x5, ReLU \\
        BatchNorm & - & Yes & Yes \\
        Attention & - & CBAM (channel + spatial) & Channel Attention \\
        Dropout & - & - & 0.1 \\
        Pooling (1st) & AvgPool 2x2 & AvgPool 2x2 & AvgPool 2x2 \\
        Conv2D (2nd) & 16 filters, 5x5, ReLU & 16 filters, 5x5, ReLU & 16 filters, 5x5, ReLU \\
        BatchNorm & - & Yes & Yes \\
        Attention & - & CBAM (channel + spatial) & Channel Attention \\
        Dropout & - & - & 0.1 \\
        Pooling (2nd) & AvgPool 2x2 & AvgPool 2x2 & AvgPool 2x2 \\
        Flatten & 13,456 features & 13,456 features & 13,456 features \\
        Dense (1st) & 120 units, ReLU & 120 units, ReLU & 120 units, ReLU, L2 (0.001) \\
        Dropout & - & - & 0.5 \\
        Dense (2nd) & 84 units, ReLU & 84 units, ReLU & 84 units, ReLU, L2 (0.001) \\
        Dropout & - & - & 0.5 \\
        Output & 10 units, softmax & 10 units, softmax & 10 units, softmax \\
        Total Params & $\sim$1.62M & $\sim$1.63M & 1,629,144 \\
        \bottomrule
    \end{tabular}
\end{table}

\subsection{Training Process and Hyperparameters}
The models were trained on the free-spoken-digits dataset, meticulously split into training (80\%, approximately 2400 samples), validation (10\%, approximately 300 samples), and test sets (10\%, approximately 300 samples). This partitioning ensured a robust evaluation framework, leveraging a Kaggle T4 GPU with single-GPU training to circumvent collective operation errors encountered with multi-GPU setups. The training process was a journey of optimization, overcoming initial challenges to achieve stable and high-performing models.

\subsubsection{Data Preprocessing}
The preprocessing pipeline was a critical component, transforming raw audio-derived spectrograms into a format suitable for CNN training. Key steps included:
\begin{itemize}
    \item \textbf{Rescaling}: Pixel values were normalized to the range [0, 1] by dividing by 255, ensuring consistency across all datasets and facilitating convergence during training. This step was essential to align the input data with the activation functions' expected range, preventing saturation and promoting stable gradient flow.
    \item \textbf{Noise Injection (Training Only)}:\\ Gaussian noise with a standard deviation of \texttt{noise\_factor=0.02} was added exclusively to the training data. This augmentation simulated real-world audio variations (e.g., background noise, microphone distortions), enhancing the model's robustness without compromising the clean validation and test sets, which served as a pure evaluation metric. The choice of a low noise factor ensured spectrograms remained interpretable, preserving essential digit patterns.
    \item \textbf{Caching and Prefetching}: These optimizations were applied to all datasets to minimize data loading bottlenecks, ensuring the GPU remained fully utilized and reducing training time per epoch. Caching stored preprocessed data in memory, while prefetching overlapped data loading with computation, maximizing efficiency.
\end{itemize}

\subsubsection{Training Hyperparameters}
The training process was guided by a carefully selected set of hyperparameters, detailed in Table \ref{tab:hyperparameters_partb}. These choices were informed by initial experiments and iterative tuning to balance stability, accuracy, and efficiency:
\begin{itemize}
    \item \textbf{Optimizer}: The Adam optimizer was chosen for its adaptive learning rate capabilities, set to a conservative 0.0001 to ensure gradual weight updates and prevent overshooting. Gradient clipping with a norm limit of 1.0 was implemented to mitigate exploding gradients, a common issue in deep networks, particularly in attention layers where feature weighting can amplify gradients.
    \item \textbf{Loss Function}: Sparse categorical crossentropy was selected, ideal for multi-class classification with integer labels, providing a smooth loss surface for optimization. This choice aligned with the task's requirements, ensuring accurate backpropagation of errors across the 10 classes.
    \item \textbf{Epochs}: Training was configured for 15 epochs, with early stopping activated if the validation loss plateaued for 3 epochs, restoring the best weights to avoid overfitting and preserve peak performance. This approach allowed the model to learn thoroughly while safeguarding against degradation.
    \item \textbf{Batch Size}: Set to 16, this value struck a balance between memory constraints on the T4 GPU and sufficient gradient updates per epoch, ensuring efficient computation without overwhelming the hardware.
    \item \textbf{Regularization (Hybrid Attention Only)}:
        \begin{itemize}
            \item \textbf{Dropout}: Applied at 0.1 after attention layers to introduce stochasticity and prevent co-adaptation of features, and 0.5 in dense layers to combat overfitting in the fully connected section. These rates were tuned to allow sufficient learning while preventing memorization.
            \item \textbf{L2 Regularization}: Set to 0.001 in dense layers, penalizing large weights to encourage a smoother decision boundary and enhance generalization. This regularization was crucial in managing the model's 1.6 million parameters against the relatively small dataset.
        \end{itemize}
    \item \textbf{Precision}: Switched to \texttt{float32} after initial runs with \texttt{float16} resulted in \texttt{inf} loss, ensuring numerical stability across all computations. This decision was pivotal, as \texttt{float16} introduced overflow/underflow in attention and dense layers, a lesson in precision selection for complex architectures.
\end{itemize}

\begin{table}[h]
    \centering
    \caption{Training Hyperparameters for All Models}
    \label{tab:hyperparameters_partb}
    \begin{tabular}{lc}
        \toprule
        \textbf{Hyperparameter} & \textbf{Value} \\
        \midrule
        Optimizer & Adam \\
        Learning Rate & 0.0001 \\
        Gradient Clipping & clipnorm=1.0 \\
        Loss Function & Sparse Categorical Crossentropy \\
        Epochs & 15 (with early stopping, patience=3) \\
        Batch Size & 16 \\
        Dropout (Hybrid Attention) & 0.1 (attention), 0.5 (dense) \\
        L2 Regularization (Hybrid Attention) & 0.001 \\
        Precision & Float32 \\
        Noise Factor (Training) & 0.02 \\
        \bottomrule
    \end{tabular}
\end{table}

\subsubsection{Training Challenges and Solutions}
The training process was not without hurdles. Early attempts using \texttt{float16} precision led to catastrophic \texttt{inf} loss, attributed to numerical overflow in the attention layers and dense computations. This instability was a significant roadblock, prompting a shift to \texttt{float32} precision, which restored numerical integrity and enabled reliable training. Additionally, the initial inclusion of a full CBAM with spatial attention introduced excessive computational complexity and instability, necessitating the simplification to channel attention in the Hybrid Attention model. Gradient clipping was a pivotal solution, capping gradient magnitudes to prevent explosive updates, while early stopping ensured the model reverted to the best-performing weights when validation loss stagnated. The journey underscored the importance of iterative debugging, with preprocessing fixes—such as resolving black validation images through dataset regeneration and validation checks—proving crucial for valid loss computation and effective generalization.

\subsection{Performance Comparison}
The performance of the three models was meticulously evaluated on the test set, with results presented in Table \ref{tab:performance_partb}. Visual representations of training and validation accuracy/loss over epochs are provided in Figure \ref{fig:plots}, offering a dynamic view of model learning, while Figure \ref{fig:attention_map} showcases the attention mechanism's focus through a sample attention map.

\begin{table}[h]
    \centering
    \caption{Final Performance Comparison of Models}
    \label{tab:performance_partb}
    \begin{tabular}{lcc}
        \toprule
        \textbf{Model} & \textbf{Training Time (s)} & \textbf{Test Accuracy} \\
        \midrule
        Baseline CNN & 126.41 & 0.9727 \\
        CNN with Attention & 310.01 & 0.9922 \\
        Hybrid Attention & 137.72 & 0.9844 \\
        \bottomrule
    \end{tabular}
\end{table}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{accuracy_loss_plots.png}
    \caption{Training and Validation Accuracy/Loss Plots for All Models. The left subplot illustrates accuracy trends, showcasing how each model learns over epochs, while the right subplot depicts loss trajectories, highlighting convergence behavior and potential overfitting. These plots provide a visual confirmation of the models' learning dynamics and generalization capabilities.}
    \label{fig:plots}
\end{figure}


\newpage
\subsubsection{Accuracy Analysis}
The test accuracy results reveal a clear hierarchy of performance, reflecting the architectural enhancements:
\begin{itemize}
    \item \textbf{Baseline CNN}: Achieved a test accuracy of 97.27\%, establishing a robust foundation. This model effectively captures broad spectrogram patterns through its convolutional and pooling layers, but its lack of attention limits its ability to prioritize specific features, leaving room for improvement. The high accuracy suggests the baseline is a strong starting point, yet it may miss subtle digit-specific cues.
    \item \textbf{CNN with Attention}: Attained the pinnacle of performance with a test accuracy of 99.22\%, a remarkable 1.95\% improvement over the baseline. The full CBAM mechanism empowers the model to dynamically weight frequency bands via channel attention and pinpoint critical time-frequency regions via spatial attention. This dual focus enables the model to discern subtle distinctions between digits (e.g., the short duration of ``1'' versus the prolonged articulation of ``7''), driving its superior accuracy and showcasing the power of comprehensive attention.
    \item \textbf{Hybrid Attention}: Secured a test accuracy of 98.44\%, surpassing the baseline by 1.17\% but trailing the full attention model by 0.78\%. The simplified channel attention mechanism concentrates on frequency bands, enhancing the model's sensitivity to digit-specific spectral characteristics. While it lacks the spatial context of CBAM, its stability and efficiency make it a compelling alternative, demonstrating that a focused approach can still yield excellent results.
\end{itemize}

\subsubsection{Training Time Analysis}
The training times underscore the computational trade-offs inherent in each model:
\begin{itemize}
    \item \textbf{Baseline CNN}: Recorded the fastest training time at 126.41 seconds, a testament to its streamlined design. Without attention layers, it minimizes computational overhead, making it the most efficient option for resource-constrained settings. This efficiency comes at the cost of slightly lower accuracy, reflecting a trade-off between speed and performance.
    \item \textbf{CNN with Attention}: Emerged as the slowest at 310.01 seconds, approximately 2.45 times longer than the baseline. The spatial attention component, with its 7x7 convolution, introduces significant computational demands, processing concatenated feature maps to refine spatial focus. This extended duration is a worthwhile trade-off for the 1.95\% accuracy gain, particularly for applications prioritizing precision over speed.
    \item \textbf{Hybrid Attention}: Clocked in at 137.72 seconds, a mere 9\% increase over the baseline. The elimination of spatial attention reduces the computational burden, allowing the model to maintain high accuracy (98.44\%) with minimal overhead. This efficiency highlights the success of the simplification strategy, offering a practical balance between performance and resource use, ideal for real-world deployment.
\end{itemize}

\subsection{Impact of Attention Mechanisms}
Attention mechanisms revolutionize CNNs by directing focus toward the most informative features in spectrogram images, a critical advantage in spoken digit recognition. This section provides an in-depth analysis of their influence on accuracy and training time, enriched with observations from the experimental journey and supported by attention map visualizations.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{attention_map.png}
    \caption{Attention Map Visualization for a Sample Spectrogram. This figure overlays importance scores on a spectrogram image, generated using Grad-CAM or channel attention weights from the CNN Attention model, highlighting the regions (e.g., frequency bands or time-frequency interactions) the model focuses on to predict a digit. The map offers a direct visual insight into the attention mechanism's effectiveness.}
    \label{fig:attention_map}
\end{figure}

\subsubsection{Accuracy Improvement}
\begin{itemize}
    \item \textbf{CNN with Attention}: The 1.95\% accuracy boost over the baseline (99.22\% vs. 97.27\%) is a striking endorsement of CBAM's efficacy. Channel attention dynamically weights frequency bands, amplifying those most indicative of digit identity (e.g., the low-frequency emphasis in ``0'' or the harmonic richness of ``3''), while spatial attention highlights key time-frequency interactions (e.g., the transient onset of ``1'' or the sustained energy of ``5''). The attention map (Figure \ref{fig:attention_map}) reveals concentrated focus on these regions, explaining the model's near-perfect performance and its ability to extract nuanced patterns.
    \item \textbf{Hybrid Attention}: The 1.17\% improvement (98.44\% vs. 97.27\%) affirms the value of even a simplified channel attention mechanism. By focusing solely on frequency bands, it enhances the model's ability to discern digit-specific spectral signatures (e.g., the distinct pitch of ``2''), though it sacrifices the spatial context provided by CBAM. The attention map shows a more uniform weighting across frequency axes, reflecting the channel-only approach, yet still delivers a significant accuracy gain over the baseline.
\end{itemize}

\subsubsection{Training Time Trade-Off}
\begin{itemize}
    \item \textbf{CNN with Attention}: The full attention mechanism extends training time by 145\% (310.01s vs. 126.41s), driven by the spatial attention's 7x7 convolution. This operation processes large feature maps, significantly increasing computational cost as it refines spatial focus across the spectrogram. While this investment yields the highest accuracy, it may be prohibitive for time-sensitive applications or environments with limited resources, necessitating a careful cost-benefit analysis based on deployment needs.
    \item \textbf{Hybrid Attention}: The simplified mechanism incurs only a 9\% overhead (137.72s vs. 126.41s), a remarkable efficiency gain. By eschewing spatial attention, it minimizes computational demands while retaining nearly the same accuracy as the full attention model (98.44\% vs. 99.22\%). The attention map confirms that channel attention alone suffices for most feature emphasis, making it an attractive option for practical deployment where speed is a priority.
\end{itemize}

\subsubsection{Stability Considerations}
The journey to stable training was fraught with challenges. The CNN with Attention initially succumbed to numerical instability, with \texttt{inf} loss emerging due to \texttt{float16} precision and the intricate spatial attention computations. This instability threatened the model's viability, prompting a strategic shift to \texttt{float32} precision in the Hybrid Attention model. By focusing on channel attention and leveraging gradient clipping, the Hybrid Attention model achieved stability without sacrificing significant accuracy, offering a robust alternative to the more complex CBAM approach. The attention map further validates this stability, showing consistent feature weighting without the erratic patterns seen in early unstable runs.

\subsection{Insights and Observations}
The experimental process yielded a wealth of insights, each illuminating different facets of the model development and training journey:
\begin{itemize}
    \item \textbf{Numerical Stability is Critical}: Early experiments using \texttt{float16} precision resulted in catastrophic \texttt{inf} loss, a stark reminder of the fragility of low-precision computations in attention-based models. Transitioning to \texttt{float32} was a game-changer, eliminating overflow/underflow issues and ensuring reliable gradient updates. This lesson underscores the need for precision selection tailored to the model's complexity, a critical consideration for future deep learning projects.
    \item \textbf{Attention Enhances Feature Focus}: The superior performance of both attention models over the baseline (97.27\%)—with the CNN with Attention reaching 99.22\% and the Hybrid Attention achieving 98.44\%—underscores the power of attention mechanisms. These models excel by prioritizing relevant spectrogram features, such as the frequency bands that distinguish ``0'' from ``8'' or the temporal dynamics of ``1''. The attention map vividly illustrates this focus, revealing concentrated weights on digit-specific regions, a testament to attention's transformative potential.
    \item \textbf{Trade-Off Between Complexity and Stability}: The full CBAM implementation offered the highest accuracy (99.22\%) but at a steep cost—145\% longer training time (310.01s) and initial instability with \texttt{float16}. The Hybrid Attention model's simplified approach, with only a 9\% time increase (137.72s) and stable performance, revealed a critical trade-off. This balance between complexity and stability is a key takeaway for designing practical deep learning models, especially when computational resources are limited.
    \item \textbf{Noise Augmentation Improves Robustness}: The introduction of noise (\texttt{noise\_factor=0.02}) to training data proved transformative, contributing to the Hybrid Attention model's high test accuracy (98.44\%) despite clean validation/test sets. This augmentation simulated real-world audio distortions, enhancing the model's resilience and preventing overfitting to pristine spectrograms. The attention map shows that noise-augmented training reinforced the model's focus on robust features, a strategy that paid dividends in generalization.
    \item \textbf{Preprocessing Challenges}: Early runs were plagued by black validation images, a perplexing issue traced to preprocessing errors, such as incorrect rescaling or corrupted image files. Resolving this through meticulous dataset regeneration and validation checks was pivotal, ensuring valid loss computation and enabling the model to generalize effectively. This experience highlighted the often-overlooked importance of data integrity, a lesson that shaped the entire training process.
    \item \textbf{Regularization Prevents Overfitting}: The strategic use of dropout (0.5 in dense layers, 0.1 after attention) and L2 regularization (0.001) was a resounding success. These techniques curbed overfitting, as evidenced by the close alignment of training and validation accuracies in the final run (e.g., 98.44\% test accuracy with minimal variance). The attention map further supports this, showing consistent feature emphasis across datasets, underscoring regularization's role in achieving robust generalization.
\end{itemize}

\subsection{Suggestions for Future Improvements}
The insights gained from this study pave the way for a range of exciting future improvements, each designed to push the boundaries of performance and practicality:
\begin{itemize}
    \item \textbf{Reintroduce Self-Attention with Stability Fixes}: The Hybrid Attention model sacrificed self-attention for stability. Future work could reintroduce it, leveraging advanced techniques like dynamic loss scaling, enhanced gradient clipping, or strict adherence to \texttt{float32} precision. This could capture global spectrogram dependencies (e.g., across time-frequency axes), potentially elevating accuracy beyond 99.22\% while maintaining stability. The attention map could guide this effort by identifying regions where self-attention might add value.
    \item \textbf{Explore Different Attention Types}: The current study focused on CBAM and its simplified variant. Exploring alternative mechanisms, such as Squeeze-and-Excitation (SE) blocks for channel-specific weighting or Transformer-based attention for sequence modeling, could offer new perspectives. These approaches might reveal additional feature hierarchies or temporal patterns, enriching the model's discriminative power, with attention maps providing visual validation.
    \item \textbf{Tune Hyperparameters}: The selected hyperparameters (e.g., learning rate 0.0001, dropout 0.5) were effective but not exhaustive. A grid search or Bayesian optimization could fine-tune the learning rate (e.g., 0.00005 to 0.001), dropout rates (e.g., 0.3 to 0.7), and L2 regularization (e.g., 0.0005 to 0.01), potentially uncovering an optimal configuration that maximizes accuracy and minimizes training time. Attention maps could help correlate these adjustments with feature focus.
    \item \textbf{Enhance Data Augmentation}: The noise augmentation (\texttt{noise\_factor=0.02}) was a success, but additional techniques could amplify robustness. Random cropping, small rotations (e.g., ±10°), or frequency masking could simulate a broader range of audio distortions, preparing the model for real-world variability and potentially boosting generalization. The impact of these augmentations could be visualized in updated attention maps.
    \item \textbf{Larger Dataset}: The free-spoken-digits dataset, with its $\sim$3000 samples, is a valuable resource but limited in diversity. Expanding with a larger dataset or generating synthetic spectrograms (e.g., via pitch shifting or speaker synthesis) could expose the model to more varied patterns, reducing overfitting and enhancing performance on unseen data. Attention maps could reveal how new data influences feature focus.
    \item \textbf{Multi-GPU Training}: The switch to single-GPU training resolved collective operation errors, but leveraging Kaggle's T4x2 GPUs could slash training time for complex models like the CNN with Attention. Future efforts should address synchronization issues (e.g., using synchronized batch normalization) to harness parallel processing, making large-scale experiments more feasible. Attention map generation could benefit from faster training cycles.
    \item \textbf{Feature Visualization}: Employing Grad-CAM or similar techniques could illuminate which spectrogram regions the attention mechanisms prioritize. This visualization would offer a deeper understanding of the model's decision-making process, validating the effectiveness of channel attention and guiding further refinements. The current attention map (Figure \ref{fig:attention_map}) is a starting point, but expanded visualizations could enhance this analysis.
\end{itemize}

\subsection{Conclusion for Part B}
This study stands as a testament to the transformative potential of attention mechanisms in spoken digit recognition, with the CNN with Attention achieving an exemplary test accuracy of 99.22\% and the Hybrid Attention model delivering a commendable 98.44\% accuracy in just 137.72 seconds of training time. The introduction of noise injection augmentation and a simplified channel attention mechanism as creative improvements not only enhanced model robustness but also ensured numerical stability, addressing the initial challenges of \texttt{inf} loss and black validation images. The experiments illuminated the critical interplay between numerical precision, preprocessing integrity, and regularization, offering a blueprint for successful deep learning endeavors. The Baseline CNN, with its 97.27\% accuracy, provided a solid foundation, while the attention-enhanced models elevated performance to near-perfect levels, albeit with varying computational costs. The attention map (Figure \ref{fig:attention_map}) provided a visual confirmation of the models' focus, reinforcing the efficacy of these enhancements. Looking ahead, the proposed improvements—ranging from advanced attention mechanisms to enriched datasets—promise to further refine this approach, paving the way for even more robust and efficient models in future explorations.

\section{Overall Conclusions and Comparisons}

This comprehensive study has demonstrated the profound impact of attention mechanisms on neural network performance across two distinct recognition tasks: handwritten digit classification (Part A) and spoken digit recognition (Part B). Through meticulous experimentation with various attention architectures, several key insights emerge from comparing these two problem domains:

\subsection{Cross-Domain Effectiveness of Attention}

The integration of attention mechanisms yielded significant performance improvements in both visual and audio recognition tasks, though with notable differences in magnitude and mechanism preference:

\begin{itemize}
    \item \textbf{Visual Recognition (MNIST)}: The SE Channel Attention CNN achieved a modest yet significant improvement of 0.45\% over the baseline (98.55\% vs. 98.10\%), with channel attention proving most effective due to the dataset's centered, low-resolution nature.
    
    \item \textbf{Audio Recognition (Spoken Digits)}: The full CBAM attention model demonstrated a more dramatic improvement of 1.95\% over the baseline (99.22\% vs. 97.27\%), with the dual channel-spatial attention mechanism excelling at capturing the complex time-frequency patterns in spectrograms.\\
\end{itemize}
This disparity suggests that attention mechanisms offer greater benefit for tasks with richer feature interactions, such as spectrograms, where temporal and frequency dimensions create a more complex feature space compared to the static, centered digits of Reduced MNIST.

\subsection{Computational Efficiency Trade-offs}

Both studies revealed important efficiency considerations, though with different optimal solutions:

\begin{itemize}
    \item \textbf{Visual Recognition}: The SE Channel Attention model required only 7.7\% additional training time compared to the baseline (44.61s vs. 41.41s), offering an excellent accuracy-efficiency balance.
    
    \item \textbf{Audio Recognition}: The full attention model demanded 145\% more training time than the baseline (310.01s vs. 126.41s), while the Hybrid Attention model achieved a compelling middle ground with a mere 9\% time increase (137.72s) and 98.44\% accuracy.\\
\end{itemize}
These findings highlight how task complexity influences the computational cost-effectiveness of attention mechanisms, with simpler datasets benefiting from lightweight attention approaches while complex feature spaces may justify more elaborate attention architectures despite their higher computational demands.

\subsection{Stability and Regularization Insights}

Both studies emphasized the critical role of regularization and numerical stability in attention-enhanced networks:

\begin{itemize}
    \item \textbf{Visual Recognition}: A uniform dropout rate of 0.3 proved sufficient across all models to minimize overfitting, with the simple dataset requiring minimal additional regularization.
    
    \item \textbf{Audio Recognition}: The spoken digit models required more extensive regularization strategies, including precision management (float32), gradient clipping, variable dropout rates (0.1 after attention, 0.5 in dense layers), and L2 regularization (0.001) to achieve stability.\\
\end{itemize}
This contrast demonstrates that attention mechanisms in more complex feature spaces (spectrograms) introduce greater instability risks, necessitating more sophisticated regularization approaches than simpler visual tasks.

\subsection{Domain-Specific Optimizations}

Each recognition domain benefited from targeted optimizations:

\begin{itemize}
    \item \textbf{Visual Recognition}: Channel-focused attention (SE) proved optimal for Reduced MNIST, leveraging the consistent spatial layout of centered digits while minimizing computational overhead.
    
    \item \textbf{Audio Recognition}: A hybrid approach balancing channel attention with noise augmentation offered the best performance-efficiency trade-off, addressing the temporal variability of spoken digits.\\
\end{itemize}
These domain-specific findings emphasize the importance of tailoring attention architectures to the inherent characteristics of the data, rather than applying a one-size-fits-all approach.

\subsection{Future Directions}

The comparative analysis suggests several promising research directions:

\begin{itemize}
    \item \textbf{Cross-Modal Attention Transfer}: Investigating whether attention mechanisms optimized for one domain (e.g., SE for visual tasks) can be effectively adapted to another (e.g., audio recognition) could reveal fundamental principles about feature importance across modalities.
    
    \item \textbf{Hybrid Attention Architectures}: Developing unified attention frameworks that can dynamically adjust their focus between channel and spatial dimensions based on the task requirements could advance the field beyond domain-specific optimizations.
    
    \item \textbf{Attention with Data Augmentation}: Further exploring the synergy between attention mechanisms and data augmentation techniques, which showed particular promise in the spoken digit task, could enhance model robustness across both domains.\\
\end{itemize}
In conclusion, this comprehensive study not only validates the effectiveness of attention mechanisms across visual and audio recognition tasks but also provides valuable insights into their optimal implementation for different data types. The significant improvements achieved—particularly in the more complex audio domain—underscore attention's transformative potential in enhancing neural network performance, while the domain-specific findings offer a nuanced understanding of how to best leverage these powerful mechanisms for different recognition challenges.

\bibliographystyle{plain}
\begin{thebibliography}{9}
\bibitem{free_spoken_digits}
Alan Chalker,
\textit{Free Spoken Digits Dataset},
2023,
\url{https://www.kaggle.com/datasets/alanchn31/free-spoken-digits}.
\bibitem{cbam_paper}
Sanghyun Woo, Jongchan Park, Joon-Young Lee, In So Kweon,
\textit{CBAM: Convolutional Block Attention Module},
2018,
\url{https://arxiv.org/abs/1807.06521}.
\bibitem{se_paper}
Jie Hu, Li Shen, Gang Sun,
\textit{Squeeze-and-Excitation Networks},
2018,
\url{https://arxiv.org/abs/1709.01507}.
\end{thebibliography}

\end{document}